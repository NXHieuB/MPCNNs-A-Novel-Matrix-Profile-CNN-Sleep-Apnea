{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"NOTES: Batch data is different each time in keras, which result in slight differences in results.\"\"\"\n",
    "\"\"\"Bettycxh, \"Bottleneck-Attention-Based-Fusion-Network-for-Sleep-Apnea-Detection,\n",
    "\" GitHub repository, n.d. [Online]. Available: https://github.com/Bettycxh/Bottleneck-Attention-Based-Fusion-Network-for-Sleep-Apnea-Detection\n",
    "\"\"\"\n",
    "import time\n",
    "import pickle\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras.layers import Conv1D, Dense, Dropout, MaxPooling1D,Reshape,multiply,Permute,\\\n",
    "              GlobalAveragePooling1D,BatchNormalization,Flatten,UpSampling1D,Conv1DTranspose,\\\n",
    "                Flatten,  Lambda, Input\n",
    "from keras.models import Model,load_model\n",
    "from keras.regularizers import l2\n",
    "from scipy.interpolate import splev, splrep\n",
    "from keras.activations import sigmoid\n",
    "from keras.callbacks import LearningRateScheduler,ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from IPython.display import SVG,display,HTML\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import confusion_matrix,f1_score,roc_auc_score\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import CubicSpline\n",
    "def interpolate_numpy_array(arr, desired_length):\n",
    "    cs = CubicSpline(np.linspace(0, 1, len(arr)), arr)\n",
    "    x_new = np.linspace(0, 1, desired_length)\n",
    "    interpolated_arr = cs(x_new)\n",
    "    return interpolated_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmu-BHHmm8I_"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 507,
     "status": "ok",
     "timestamp": 1693229193098,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "IslyMKFQm8JB"
   },
   "outputs": [],
   "source": [
    "base_dir = \"dataset\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "ir = 3 # interpolate interval\n",
    "before = 2\n",
    "after = 2\n",
    "# normalize\n",
    "scaler = lambda arr: (arr - np.min(arr)) / (np.max(arr) - np.min(arr))\n",
    "def load_data():\n",
    "    tm = np.arange(0, (before + 1 + after) * 60, step=1 / float(ir))\n",
    "\n",
    "    with open(os.path.join(base_dir, \"T_1.pkl\"), 'rb') as f: # read preprocessing result\n",
    "        apnea_ecg = pickle.load(f)\n",
    "    x,x_train,x_val = [],[],[]\n",
    "    o_train, y_train = apnea_ecg[\"o_train\"], apnea_ecg[\"y_train\"]\n",
    "    groups_train = apnea_ecg[\"groups_train\"]\n",
    "    for i in range(len(o_train)):\n",
    "        min_distance_list, max_distance_list, mean_distance_list = o_train[i]\n",
    "\t\t# Curve interpolation\n",
    "        min_distance_list_inter = interpolate_numpy_array(min_distance_list,900)\n",
    "        max_distance_list_inter = interpolate_numpy_array(max_distance_list,900)\n",
    "        mean_distance_list_inter = interpolate_numpy_array(mean_distance_list,900)\n",
    "        #We can change the value of min_distance_list_inter, max_distance_list_inter and mean_distance_list_inter \n",
    "        #to test first ablation experiments\n",
    "        x.append([min_distance_list_inter, max_distance_list_inter])\n",
    "    groups_training,groups_val=[],[]\n",
    "\n",
    "    num=[i for i in range(16713)]\n",
    "    trainlist, vallist,y_train, y_val = train_test_split(num,y_train, test_size=0.3,random_state=42,stratify =y_train)\n",
    "    print()\n",
    "    for i in trainlist:\n",
    "        x_train.append(x[i])\n",
    "        groups_training.append(groups_train[i])\n",
    "    for i in vallist:\n",
    "        x_val.append(x[i])\n",
    "        groups_val.append(groups_train[i])\n",
    "\n",
    "    x_train = np.array(x_train, dtype=\"float32\").transpose((0, 2, 1)) # convert to numpy format\n",
    "    y_train= np.array(y_train, dtype=\"float32\")\n",
    "    x_val = np.array(x_val, dtype=\"float32\").transpose((0, 2, 1)) # convert to numpy format\n",
    "    y_val = np.array(y_val, dtype=\"float32\")\n",
    "\n",
    "    x_test = []\n",
    "    o_test, y_test = apnea_ecg[\"o_test\"], apnea_ecg[\"y_test\"]\n",
    "    groups_test = apnea_ecg[\"groups_test\"]\n",
    "    for i in range(len(o_test)):\n",
    "        min_distance_list, max_distance_list, standard_deviation_distance_list = o_test[i]\n",
    "\t\t# Curve interpolation\n",
    "        min_distance_list_inter = interpolate_numpy_array(min_distance_list,900)\n",
    "        max_distance_list_inter = interpolate_numpy_array(max_distance_list,900)\n",
    "        mean_distance_list_inter = interpolate_numpy_array(mean_distance_list,900)\n",
    "        x_test.append([min_distance_list_inter, max_distance_list_inter])\n",
    "    x_test = np.array(x_test, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_test = np.array(y_test, dtype=\"float32\")\n",
    "\n",
    "    return x_train,y_train, groups_training,x_val, y_val, groups_val, x_test, y_test, groups_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35872,
     "status": "ok",
     "timestamp": 1693229233286,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "7pJOzNkzm8JC",
    "outputId": "8e7e9cc5-520c-49c5-e7fe-cae0037c9d4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input_shape (11699, 900, 2)\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train, groups_train,x_val, y_val, groups_val, x_test, y_test, groups_test= load_data()\n",
    "y_train = np_utils.to_categorical(y_train, num_classes=2) # Convert to two categories\n",
    "y_val = np_utils.to_categorical(y_val, num_classes=2)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes=2)\n",
    "print('input_shape',x_train.shape)\n",
    "#rri_train: min_distance_list_inter\n",
    "#ampl_train: max_distance_list_inter\n",
    "#We only change the input in the original \n",
    "rri_train=np.expand_dims(x_train[:,:,0],axis=2)\n",
    "ampl_train=np.expand_dims(x_train[:,:,1],axis=2)\n",
    "rri_val=np.expand_dims(x_val[:,:,0],axis=2)\n",
    "ampl_val=np.expand_dims(x_val[:,:,1],axis=2)\n",
    "rri_test=np.expand_dims(x_test[:,:,0],axis=2)\n",
    "ampl_test=np.expand_dims(x_test[:,:,1],axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MADDevGm8JC"
   },
   "source": [
    "# BAFNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 903,
     "status": "ok",
     "timestamp": 1693229249510,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "7-oHdvBkm8JD"
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 return_attention=False,\n",
    "                 history_only=False,\n",
    "                 **kwargs):\n",
    "        super(ScaledDotProductAttention, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.history_only = history_only\n",
    "        self.intensity = self.attention = None\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'return_attention': self.return_attention,\n",
    "            'history_only': self.history_only,\n",
    "        }\n",
    "        base_config = super(ScaledDotProductAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if isinstance(input_shape, list):\n",
    "            query_shape, key_shape, value_shape = input_shape\n",
    "        else:\n",
    "            query_shape = key_shape = value_shape = input_shape\n",
    "        output_shape = query_shape[:-1] + value_shape[-1:]\n",
    "        if self.return_attention:\n",
    "            attention_shape = query_shape[:2] + (key_shape[1],)\n",
    "            return [output_shape, attention_shape]\n",
    "        return output_shape\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if isinstance(mask, list):\n",
    "            mask = mask[0]\n",
    "        if self.return_attention:\n",
    "            return [mask, None]\n",
    "        return mask\n",
    "\n",
    "    def call(self, inputs, mask=None, **kwargs):\n",
    "        if isinstance(inputs, list):\n",
    "            query, key, value = inputs\n",
    "        else:\n",
    "            query = key = value = inputs\n",
    "        if isinstance(mask, list):\n",
    "            mask = mask[1]\n",
    "        feature_dim = K.shape(query)[-1]\n",
    "        e = K.batch_dot(query, key, axes=2) / K.sqrt(K.cast(feature_dim, dtype=K.floatx()))\n",
    "        if self.history_only:\n",
    "            query_len, key_len = K.shape(query)[1], K.shape(key)[1]\n",
    "            indices = K.expand_dims(K.arange(0, key_len), axis=0)\n",
    "            upper = K.expand_dims(K.arange(0, query_len), axis=-1)\n",
    "            e -= 10000.0 * K.expand_dims(K.cast(indices > upper, K.floatx()), axis=0)\n",
    "        if mask is not None:\n",
    "            e -= 10000.0 * (1.0 - K.cast(K.expand_dims(mask, axis=-2), K.floatx()))\n",
    "        self.intensity = e\n",
    "        e = K.exp(e - K.max(e, axis=-1, keepdims=True))\n",
    "        self.attention = e / K.sum(e, axis=-1, keepdims=True)\n",
    "        v = K.batch_dot(self.attention, value)\n",
    "        if self.return_attention:\n",
    "            return [v, self.attention]\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1693229253281,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "3lSL0Vg0m8JD"
   },
   "outputs": [],
   "source": [
    "def create_model(input_shape,weight=1e-3):\n",
    "    inp=Input(shape=input_shape)\n",
    "    input1 =Reshape((900, 1))(inp[:,:,0])\n",
    "    input2 = Reshape((900, 1))(inp[:,:,1])\n",
    "\n",
    "    x1 = Conv1D(16, kernel_size=11, strides=1, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                  kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(input1)\n",
    "    x2 = Conv1D(16, kernel_size=11, strides=1, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                  kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(input2)\n",
    "\n",
    "    x1 = Conv1D(24, kernel_size=11, strides=2, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                  kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = MaxPooling1D(pool_size=3, padding=\"same\")(x1)\n",
    "    x2 = Conv1D(24, kernel_size=11, strides=2, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                  kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = MaxPooling1D(pool_size=3, padding=\"same\")(x2)\n",
    "    fsn2=keras.layers.concatenate([x1, x2], name=\"fsn2\", axis=-1)\n",
    "\n",
    "    x1 = Conv1D(32 , kernel_size=11, strides=1, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                  kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = MaxPooling1D(pool_size=5, padding=\"same\")(x1)\n",
    "    x2 = Conv1D(32, kernel_size=11, strides=1, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                  kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = MaxPooling1D(pool_size=5, padding=\"same\")(x2)\n",
    "    fsn3=Conv1D(32, kernel_size=11, strides=1, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                  kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight))(fsn2)\n",
    "    fsn3=MaxPooling1D(pool_size=5, padding=\"same\")(fsn3)\n",
    "    fsn3=ScaledDotProductAttention()([fsn3,fsn3,fsn3])\n",
    "    x1=ScaledDotProductAttention()([fsn3,x1,x1])\n",
    "    x2=ScaledDotProductAttention()([fsn3,x2,x2])\n",
    "\n",
    "    # concat\n",
    "    concat = keras.layers.concatenate([x1, x2], name=\"Concat_Layer_x1\", axis=-1)\n",
    "\n",
    "    # FCN_1\n",
    "    FCN1 = UpSampling1D(5)(x1)\n",
    "    FCN1 = Conv1DTranspose(24, kernel_size=11, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "                              kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight))(FCN1)\n",
    "    FCN1 = UpSampling1D(3)(FCN1)\n",
    "    FCN1 = Conv1DTranspose(16, kernel_size=11, strides=2, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "                              kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight))(FCN1)\n",
    "    FCN1 = Conv1DTranspose(1, kernel_size=11, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "                              kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight),name='rri')(FCN1)\n",
    "\n",
    "    # FCN_2\n",
    "    FCN2 = UpSampling1D(5)(x2)\n",
    "    FCN2 = Conv1DTranspose(24, kernel_size=11, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "                              kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight))(FCN2)\n",
    "    FCN2 = UpSampling1D(3)(FCN2)\n",
    "    FCN2 = Conv1DTranspose(16, kernel_size=11, strides=2, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "                              kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight))(FCN2)\n",
    "    FCN2 = Conv1DTranspose(1, kernel_size=11, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "                              kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight),name='ampl')(FCN2)\n",
    "\n",
    "    #Channel-wise fusion module\n",
    "    squeeze = GlobalAveragePooling1D()(concat)\n",
    "    excitation=Dense(32,activation='relu')(squeeze)\n",
    "    excitation=Dense(64,activation='sigmoid')(excitation)\n",
    "    excitation = Reshape((1, 64))(excitation)\n",
    "    scale = multiply([concat, excitation])\n",
    "\n",
    "    # Classification\n",
    "    x = GlobalAveragePooling1D(name='GAP')(scale)\n",
    "    outputs=Dense(2,activation='softmax',name=\"outputs\")(x)\n",
    "    model = Model(inputs=inp, outputs=[outputs,FCN1,FCN2])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5Y_0rkvm8JE"
   },
   "source": [
    "# Training stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1693229257050,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "Cf6jbmt4m8JE"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 70 and (epoch - 1) % 10 == 0:\n",
    "        lr *= 0.1\n",
    "    print(\"Learning rate: \", lr)\n",
    "    return lr\n",
    "\n",
    "def plot(history):\n",
    "    \"\"\"Plot performance curve\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    print(history)\n",
    "    axes[0].plot(history[\"outputs_loss\"], \"r-\", history[\"val_outputs_loss\"], \"b-\", linewidth=0.5)\n",
    "    axes[0].set_title(\"Loss\")\n",
    "    axes[1].plot(history[\"outputs_accuracy\"], \"r-\", history[\"val_outputs_accuracy\"], \"b-\", linewidth=0.5)\n",
    "    axes[1].set_title(\"Accuracy\")\n",
    "    fig.tight_layout()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 267778,
     "status": "ok",
     "timestamp": 1693231022701,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "aFCCsyPlm8JE",
    "outputId": "0557c7b0-3ca8-49df-9364-96292982338b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 3.6678 - outputs_loss: 0.6778 - rri_loss: 0.6912 - ampl_loss: 1.7003 - outputs_accuracy: 0.5774\n",
      "Epoch 1: val_outputs_accuracy improved from -inf to 0.61249, saving model to /content/drive/MyDrive/Final Result/Final_performance/BAFNet/model/min_mean_1.hdf5\n",
      "92/92 [==============================] - 11s 44ms/step - loss: 3.6326 - outputs_loss: 0.6777 - rri_loss: 0.6815 - ampl_loss: 1.6754 - outputs_accuracy: 0.5777 - val_loss: 1.3602 - val_outputs_loss: 0.6606 - val_rri_loss: 0.0582 - val_ampl_loss: 0.0734 - val_outputs_accuracy: 0.6125 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 1.3306 - outputs_loss: 0.6603 - rri_loss: 0.0568 - ampl_loss: 0.0630 - outputs_accuracy: 0.6126\n",
      "Epoch 2: val_outputs_accuracy did not improve from 0.61249\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 1.3305 - outputs_loss: 0.6603 - rri_loss: 0.0567 - ampl_loss: 0.0630 - outputs_accuracy: 0.6126 - val_loss: 1.3046 - val_outputs_loss: 0.6578 - val_rri_loss: 0.0559 - val_ampl_loss: 0.0553 - val_outputs_accuracy: 0.6125 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 1.2867 - outputs_loss: 0.6583 - rri_loss: 0.0552 - ampl_loss: 0.0483 - outputs_accuracy: 0.6123\n",
      "Epoch 3: val_outputs_accuracy did not improve from 0.61249\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 1.2863 - outputs_loss: 0.6580 - rri_loss: 0.0552 - ampl_loss: 0.0482 - outputs_accuracy: 0.6126 - val_loss: 1.2662 - val_outputs_loss: 0.6536 - val_rri_loss: 0.0550 - val_ampl_loss: 0.0426 - val_outputs_accuracy: 0.6125 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 1.2498 - outputs_loss: 0.6498 - rri_loss: 0.0541 - ampl_loss: 0.0387 - outputs_accuracy: 0.6124\n",
      "Epoch 4: val_outputs_accuracy did not improve from 0.61249\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 1.2496 - outputs_loss: 0.6497 - rri_loss: 0.0541 - ampl_loss: 0.0387 - outputs_accuracy: 0.6126 - val_loss: 1.2275 - val_outputs_loss: 0.6360 - val_rri_loss: 0.0554 - val_ampl_loss: 0.0365 - val_outputs_accuracy: 0.6123 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 1.1915 - outputs_loss: 0.6085 - rri_loss: 0.0558 - ampl_loss: 0.0330 - outputs_accuracy: 0.6623\n",
      "Epoch 5: val_outputs_accuracy improved from 0.61249 to 0.72976, saving model to /content/drive/MyDrive/Final Result/Final_performance/BAFNet/model/min_mean_1.hdf5\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 1.1913 - outputs_loss: 0.6082 - rri_loss: 0.0558 - ampl_loss: 0.0331 - outputs_accuracy: 0.6624 - val_loss: 1.1390 - val_outputs_loss: 0.5600 - val_rri_loss: 0.0587 - val_ampl_loss: 0.0311 - val_outputs_accuracy: 0.7298 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 1.0812 - outputs_loss: 0.4990 - rri_loss: 0.0643 - ampl_loss: 0.0325 - outputs_accuracy: 0.7642\n",
      "Epoch 6: val_outputs_accuracy improved from 0.72976 to 0.79996, saving model to /content/drive/MyDrive/Final Result/Final_performance/BAFNet/model/min_mean_1.hdf5\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 1.0808 - outputs_loss: 0.4986 - rri_loss: 0.0642 - ampl_loss: 0.0325 - outputs_accuracy: 0.7643 - val_loss: 1.0199 - val_outputs_loss: 0.4392 - val_rri_loss: 0.0645 - val_ampl_loss: 0.0348 - val_outputs_accuracy: 0.8000 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.9388 - outputs_loss: 0.3658 - rri_loss: 0.0596 - ampl_loss: 0.0347 - outputs_accuracy: 0.8447\n",
      "Epoch 7: val_outputs_accuracy improved from 0.79996 to 0.86578, saving model to /content/drive/MyDrive/Final Result/Final_performance/BAFNet/model/min_mean_1.hdf5\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.9387 - outputs_loss: 0.3657 - rri_loss: 0.0595 - ampl_loss: 0.0347 - outputs_accuracy: 0.8446 - val_loss: 0.8841 - val_outputs_loss: 0.3242 - val_rri_loss: 0.0576 - val_ampl_loss: 0.0270 - val_outputs_accuracy: 0.8658 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 0.8732 - outputs_loss: 0.3165 - rri_loss: 0.0563 - ampl_loss: 0.0284 - outputs_accuracy: 0.8705\n",
      "Epoch 8: val_outputs_accuracy did not improve from 0.86578\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.8720 - outputs_loss: 0.3155 - rri_loss: 0.0563 - ampl_loss: 0.0283 - outputs_accuracy: 0.8711 - val_loss: 0.8722 - val_outputs_loss: 0.3193 - val_rri_loss: 0.0576 - val_ampl_loss: 0.0272 - val_outputs_accuracy: 0.8600 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.8375 - outputs_loss: 0.2889 - rri_loss: 0.0568 - ampl_loss: 0.0269 - outputs_accuracy: 0.8866\n",
      "Epoch 9: val_outputs_accuracy improved from 0.86578 to 0.89190, saving model to /content/drive/MyDrive/Final Result/Final_performance/BAFNet/model/min_mean_1.hdf5\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.8387 - outputs_loss: 0.2901 - rri_loss: 0.0568 - ampl_loss: 0.0269 - outputs_accuracy: 0.8861 - val_loss: 0.8182 - val_outputs_loss: 0.2792 - val_rri_loss: 0.0556 - val_ampl_loss: 0.0220 - val_outputs_accuracy: 0.8919 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.8102 - outputs_loss: 0.2732 - rri_loss: 0.0558 - ampl_loss: 0.0232 - outputs_accuracy: 0.8946\n",
      "Epoch 10: val_outputs_accuracy improved from 0.89190 to 0.89888, saving model to /content/drive/MyDrive/Final Result/Final_performance/BAFNet/model/min_mean_1.hdf5\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.8103 - outputs_loss: 0.2734 - rri_loss: 0.0558 - ampl_loss: 0.0232 - outputs_accuracy: 0.8944 - val_loss: 0.8082 - val_outputs_loss: 0.2735 - val_rri_loss: 0.0592 - val_ampl_loss: 0.0209 - val_outputs_accuracy: 0.8989 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.8004 - outputs_loss: 0.2700 - rri_loss: 0.0568 - ampl_loss: 0.0216 - outputs_accuracy: 0.8954\n",
      "Epoch 11: val_outputs_accuracy improved from 0.89888 to 0.90028, saving model to /content/drive/MyDrive/Final Result/Final_performance/BAFNet/model/min_mean_1.hdf5\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.8002 - outputs_loss: 0.2699 - rri_loss: 0.0568 - ampl_loss: 0.0216 - outputs_accuracy: 0.8955 - val_loss: 0.7931 - val_outputs_loss: 0.2692 - val_rri_loss: 0.0549 - val_ampl_loss: 0.0202 - val_outputs_accuracy: 0.9003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.7707 - outputs_loss: 0.2495 - rri_loss: 0.0551 - ampl_loss: 0.0204 - outputs_accuracy: 0.9026\n",
      "Epoch 12: val_outputs_accuracy improved from 0.90028 to 0.90606, saving model to /content/drive/MyDrive/Final Result/Final_performance/BAFNet/model/min_mean_1.hdf5\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.7704 - outputs_loss: 0.2493 - rri_loss: 0.0550 - ampl_loss: 0.0204 - outputs_accuracy: 0.9026 - val_loss: 0.7727 - val_outputs_loss: 0.2559 - val_rri_loss: 0.0554 - val_ampl_loss: 0.0187 - val_outputs_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.7690 - outputs_loss: 0.2551 - rri_loss: 0.0556 - ampl_loss: 0.0185 - outputs_accuracy: 0.9017\n",
      "Epoch 13: val_outputs_accuracy improved from 0.90606 to 0.90646, saving model to /content/drive/MyDrive/Final Result/Final_performance/BAFNet/model/min_mean_1.hdf5\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.7690 - outputs_loss: 0.2551 - rri_loss: 0.0556 - ampl_loss: 0.0185 - outputs_accuracy: 0.9017 - val_loss: 0.7660 - val_outputs_loss: 0.2532 - val_rri_loss: 0.0579 - val_ampl_loss: 0.0179 - val_outputs_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.7514 - outputs_loss: 0.2445 - rri_loss: 0.0545 - ampl_loss: 0.0183 - outputs_accuracy: 0.9071\n",
      "Epoch 14: val_outputs_accuracy improved from 0.90646 to 0.90746, saving model to /content/drive/MyDrive/Final Result/Final_performance/BAFNet/model/min_mean_1.hdf5\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.7513 - outputs_loss: 0.2444 - rri_loss: 0.0545 - ampl_loss: 0.0183 - outputs_accuracy: 0.9073 - val_loss: 0.7516 - val_outputs_loss: 0.2482 - val_rri_loss: 0.0554 - val_ampl_loss: 0.0170 - val_outputs_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.7427 - outputs_loss: 0.2411 - rri_loss: 0.0541 - ampl_loss: 0.0191 - outputs_accuracy: 0.9076\n",
      "Epoch 15: val_outputs_accuracy did not improve from 0.90746\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.7422 - outputs_loss: 0.2406 - rri_loss: 0.0541 - ampl_loss: 0.0191 - outputs_accuracy: 0.9078 - val_loss: 0.7906 - val_outputs_loss: 0.2902 - val_rri_loss: 0.0562 - val_ampl_loss: 0.0185 - val_outputs_accuracy: 0.8921 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.7406 - outputs_loss: 0.2450 - rri_loss: 0.0545 - ampl_loss: 0.0181 - outputs_accuracy: 0.9076\n",
      "Epoch 16: val_outputs_accuracy did not improve from 0.90746\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.7405 - outputs_loss: 0.2449 - rri_loss: 0.0545 - ampl_loss: 0.0181 - outputs_accuracy: 0.9075 - val_loss: 0.7763 - val_outputs_loss: 0.2842 - val_rri_loss: 0.0539 - val_ampl_loss: 0.0179 - val_outputs_accuracy: 0.8883 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.7237 - outputs_loss: 0.2345 - rri_loss: 0.0540 - ampl_loss: 0.0174 - outputs_accuracy: 0.9105\n",
      "Epoch 17: val_outputs_accuracy improved from 0.90746 to 0.91165, saving model to /content/drive/MyDrive/Final Result/Final_performance/BAFNet/model/min_mean_1.hdf5\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.7239 - outputs_loss: 0.2346 - rri_loss: 0.0540 - ampl_loss: 0.0174 - outputs_accuracy: 0.9104 - val_loss: 0.7344 - val_outputs_loss: 0.2470 - val_rri_loss: 0.0543 - val_ampl_loss: 0.0181 - val_outputs_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.7263 - outputs_loss: 0.2427 - rri_loss: 0.0539 - ampl_loss: 0.0170 - outputs_accuracy: 0.9062\n",
      "Epoch 18: val_outputs_accuracy did not improve from 0.91165\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.7265 - outputs_loss: 0.2429 - rri_loss: 0.0539 - ampl_loss: 0.0170 - outputs_accuracy: 0.9061 - val_loss: 0.7401 - val_outputs_loss: 0.2594 - val_rri_loss: 0.0538 - val_ampl_loss: 0.0168 - val_outputs_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 0.7096 - outputs_loss: 0.2317 - rri_loss: 0.0538 - ampl_loss: 0.0165 - outputs_accuracy: 0.9136\n",
      "Epoch 19: val_outputs_accuracy improved from 0.91165 to 0.91424, saving model to /content/drive/MyDrive/Final Result/Final_performance/BAFNet/model/min_mean_1.hdf5\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.7090 - outputs_loss: 0.2311 - rri_loss: 0.0538 - ampl_loss: 0.0165 - outputs_accuracy: 0.9138 - val_loss: 0.7099 - val_outputs_loss: 0.2339 - val_rri_loss: 0.0535 - val_ampl_loss: 0.0176 - val_outputs_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.7040 - outputs_loss: 0.2318 - rri_loss: 0.0540 - ampl_loss: 0.0158 - outputs_accuracy: 0.9100\n",
      "Epoch 20: val_outputs_accuracy did not improve from 0.91424\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.7038 - outputs_loss: 0.2315 - rri_loss: 0.0540 - ampl_loss: 0.0158 - outputs_accuracy: 0.9102 - val_loss: 0.7287 - val_outputs_loss: 0.2553 - val_rri_loss: 0.0553 - val_ampl_loss: 0.0181 - val_outputs_accuracy: 0.9023 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.6998 - outputs_loss: 0.2315 - rri_loss: 0.0544 - ampl_loss: 0.0161 - outputs_accuracy: 0.9123\n",
      "Epoch 21: val_outputs_accuracy improved from 0.91424 to 0.91524, saving model to /content/drive/MyDrive/Final Result/Final_performance/BAFNet/model/min_mean_1.hdf5\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.6996 - outputs_loss: 0.2313 - rri_loss: 0.0544 - ampl_loss: 0.0161 - outputs_accuracy: 0.9122 - val_loss: 0.6972 - val_outputs_loss: 0.2317 - val_rri_loss: 0.0551 - val_ampl_loss: 0.0150 - val_outputs_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.6940 - outputs_loss: 0.2319 - rri_loss: 0.0536 - ampl_loss: 0.0154 - outputs_accuracy: 0.9103\n",
      "Epoch 22: val_outputs_accuracy improved from 0.91524 to 0.91623, saving model to /content/drive/MyDrive/Final Result/Final_performance/BAFNet/model/min_mean_1.hdf5\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.6941 - outputs_loss: 0.2319 - rri_loss: 0.0537 - ampl_loss: 0.0154 - outputs_accuracy: 0.9102 - val_loss: 0.6923 - val_outputs_loss: 0.2329 - val_rri_loss: 0.0535 - val_ampl_loss: 0.0152 - val_outputs_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.6806 - outputs_loss: 0.2222 - rri_loss: 0.0540 - ampl_loss: 0.0160 - outputs_accuracy: 0.9166\n",
      "Epoch 23: val_outputs_accuracy did not improve from 0.91623\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.6806 - outputs_loss: 0.2222 - rri_loss: 0.0540 - ampl_loss: 0.0160 - outputs_accuracy: 0.9167 - val_loss: 0.7430 - val_outputs_loss: 0.2850 - val_rri_loss: 0.0540 - val_ampl_loss: 0.0180 - val_outputs_accuracy: 0.8925 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.6745 - outputs_loss: 0.2211 - rri_loss: 0.0540 - ampl_loss: 0.0156 - outputs_accuracy: 0.9165\n",
      "Epoch 24: val_outputs_accuracy did not improve from 0.91623\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.6746 - outputs_loss: 0.2212 - rri_loss: 0.0540 - ampl_loss: 0.0156 - outputs_accuracy: 0.9165 - val_loss: 0.6819 - val_outputs_loss: 0.2311 - val_rri_loss: 0.0543 - val_ampl_loss: 0.0152 - val_outputs_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.6647 - outputs_loss: 0.2154 - rri_loss: 0.0534 - ampl_loss: 0.0168 - outputs_accuracy: 0.9176\n",
      "Epoch 25: val_outputs_accuracy did not improve from 0.91623\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.6646 - outputs_loss: 0.2154 - rri_loss: 0.0534 - ampl_loss: 0.0168 - outputs_accuracy: 0.9177 - val_loss: 0.6904 - val_outputs_loss: 0.2450 - val_rri_loss: 0.0541 - val_ampl_loss: 0.0145 - val_outputs_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 0.6661 - outputs_loss: 0.2230 - rri_loss: 0.0534 - ampl_loss: 0.0152 - outputs_accuracy: 0.9156\n",
      "Epoch 26: val_outputs_accuracy did not improve from 0.91623\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.6658 - outputs_loss: 0.2227 - rri_loss: 0.0534 - ampl_loss: 0.0152 - outputs_accuracy: 0.9159 - val_loss: 0.7407 - val_outputs_loss: 0.2973 - val_rri_loss: 0.0546 - val_ampl_loss: 0.0164 - val_outputs_accuracy: 0.8817 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.6733 - outputs_loss: 0.2338 - rri_loss: 0.0537 - ampl_loss: 0.0152 - outputs_accuracy: 0.9116\n",
      "Epoch 27: val_outputs_accuracy improved from 0.91623 to 0.91803, saving model to /content/drive/MyDrive/Final Result/Final_performance/BAFNet/model/min_mean_1.hdf5\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.6728 - outputs_loss: 0.2333 - rri_loss: 0.0537 - ampl_loss: 0.0152 - outputs_accuracy: 0.9120 - val_loss: 0.6643 - val_outputs_loss: 0.2267 - val_rri_loss: 0.0539 - val_ampl_loss: 0.0155 - val_outputs_accuracy: 0.9180 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.6562 - outputs_loss: 0.2219 - rri_loss: 0.0533 - ampl_loss: 0.0148 - outputs_accuracy: 0.9166\n",
      "Epoch 28: val_outputs_accuracy did not improve from 0.91803\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.6559 - outputs_loss: 0.2216 - rri_loss: 0.0533 - ampl_loss: 0.0148 - outputs_accuracy: 0.9167 - val_loss: 0.6607 - val_outputs_loss: 0.2290 - val_rri_loss: 0.0535 - val_ampl_loss: 0.0142 - val_outputs_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.6382 - outputs_loss: 0.2078 - rri_loss: 0.0530 - ampl_loss: 0.0156 - outputs_accuracy: 0.9223\n",
      "Epoch 29: val_outputs_accuracy improved from 0.91803 to 0.91823, saving model to /content/drive/MyDrive/Final Result/Final_performance/BAFNet/model/min_mean_1.hdf5\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.6380 - outputs_loss: 0.2076 - rri_loss: 0.0530 - ampl_loss: 0.0156 - outputs_accuracy: 0.9225 - val_loss: 0.6521 - val_outputs_loss: 0.2229 - val_rri_loss: 0.0541 - val_ampl_loss: 0.0157 - val_outputs_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.6342 - outputs_loss: 0.2085 - rri_loss: 0.0533 - ampl_loss: 0.0151 - outputs_accuracy: 0.9214\n",
      "Epoch 30: val_outputs_accuracy did not improve from 0.91823\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.6339 - outputs_loss: 0.2082 - rri_loss: 0.0533 - ampl_loss: 0.0151 - outputs_accuracy: 0.9214 - val_loss: 0.6552 - val_outputs_loss: 0.2329 - val_rri_loss: 0.0530 - val_ampl_loss: 0.0145 - val_outputs_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.6401 - outputs_loss: 0.2197 - rri_loss: 0.0533 - ampl_loss: 0.0143 - outputs_accuracy: 0.9178\n",
      "Epoch 31: val_outputs_accuracy improved from 0.91823 to 0.91923, saving model to /content/drive/MyDrive/Final Result/Final_performance/BAFNet/model/min_mean_1.hdf5\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.6400 - outputs_loss: 0.2195 - rri_loss: 0.0532 - ampl_loss: 0.0143 - outputs_accuracy: 0.9179 - val_loss: 0.6397 - val_outputs_loss: 0.2201 - val_rri_loss: 0.0552 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.6250 - outputs_loss: 0.2085 - rri_loss: 0.0530 - ampl_loss: 0.0150 - outputs_accuracy: 0.9211\n",
      "Epoch 32: val_outputs_accuracy did not improve from 0.91923\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.6251 - outputs_loss: 0.2087 - rri_loss: 0.0530 - ampl_loss: 0.0150 - outputs_accuracy: 0.9210 - val_loss: 0.6523 - val_outputs_loss: 0.2393 - val_rri_loss: 0.0530 - val_ampl_loss: 0.0140 - val_outputs_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.6153 - outputs_loss: 0.2036 - rri_loss: 0.0531 - ampl_loss: 0.0145 - outputs_accuracy: 0.9244\n",
      "Epoch 33: val_outputs_accuracy did not improve from 0.91923\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.6155 - outputs_loss: 0.2038 - rri_loss: 0.0531 - ampl_loss: 0.0146 - outputs_accuracy: 0.9244 - val_loss: 0.6410 - val_outputs_loss: 0.2288 - val_rri_loss: 0.0540 - val_ampl_loss: 0.0164 - val_outputs_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.6247 - outputs_loss: 0.2087 - rri_loss: 0.0537 - ampl_loss: 0.0225 - outputs_accuracy: 0.9222\n",
      "Epoch 34: val_outputs_accuracy did not improve from 0.91923\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.6247 - outputs_loss: 0.2087 - rri_loss: 0.0537 - ampl_loss: 0.0225 - outputs_accuracy: 0.9222 - val_loss: 0.6421 - val_outputs_loss: 0.2347 - val_rri_loss: 0.0556 - val_ampl_loss: 0.0142 - val_outputs_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.6103 - outputs_loss: 0.2072 - rri_loss: 0.0530 - ampl_loss: 0.0146 - outputs_accuracy: 0.9221\n",
      "Epoch 35: val_outputs_accuracy improved from 0.91923 to 0.92062, saving model to /content/drive/MyDrive/Final Result/Final_performance/BAFNet/model/min_mean_1.hdf5\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.6100 - outputs_loss: 0.2069 - rri_loss: 0.0530 - ampl_loss: 0.0146 - outputs_accuracy: 0.9223 - val_loss: 0.6223 - val_outputs_loss: 0.2203 - val_rri_loss: 0.0539 - val_ampl_loss: 0.0147 - val_outputs_accuracy: 0.9206 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 0.6019 - outputs_loss: 0.2040 - rri_loss: 0.0525 - ampl_loss: 0.0140 - outputs_accuracy: 0.9218\n",
      "Epoch 36: val_outputs_accuracy did not improve from 0.92062\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.6018 - outputs_loss: 0.2040 - rri_loss: 0.0525 - ampl_loss: 0.0140 - outputs_accuracy: 0.9218 - val_loss: 0.6144 - val_outputs_loss: 0.2180 - val_rri_loss: 0.0532 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.5938 - outputs_loss: 0.1996 - rri_loss: 0.0526 - ampl_loss: 0.0144 - outputs_accuracy: 0.9268\n",
      "Epoch 37: val_outputs_accuracy did not improve from 0.92062\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.5938 - outputs_loss: 0.1996 - rri_loss: 0.0526 - ampl_loss: 0.0144 - outputs_accuracy: 0.9267 - val_loss: 0.6212 - val_outputs_loss: 0.2276 - val_rri_loss: 0.0527 - val_ampl_loss: 0.0158 - val_outputs_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.5949 - outputs_loss: 0.2047 - rri_loss: 0.0528 - ampl_loss: 0.0141 - outputs_accuracy: 0.9236\n",
      "Epoch 38: val_outputs_accuracy did not improve from 0.92062\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.5945 - outputs_loss: 0.2044 - rri_loss: 0.0528 - ampl_loss: 0.0141 - outputs_accuracy: 0.9237 - val_loss: 0.6077 - val_outputs_loss: 0.2191 - val_rri_loss: 0.0533 - val_ampl_loss: 0.0142 - val_outputs_accuracy: 0.9194 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.5905 - outputs_loss: 0.2046 - rri_loss: 0.0526 - ampl_loss: 0.0143 - outputs_accuracy: 0.9224\n",
      "Epoch 39: val_outputs_accuracy did not improve from 0.92062\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.5906 - outputs_loss: 0.2047 - rri_loss: 0.0526 - ampl_loss: 0.0143 - outputs_accuracy: 0.9223 - val_loss: 0.6260 - val_outputs_loss: 0.2420 - val_rri_loss: 0.0530 - val_ampl_loss: 0.0143 - val_outputs_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.5784 - outputs_loss: 0.1971 - rri_loss: 0.0526 - ampl_loss: 0.0139 - outputs_accuracy: 0.9266\n",
      "Epoch 40: val_outputs_accuracy did not improve from 0.92062\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.5787 - outputs_loss: 0.1974 - rri_loss: 0.0526 - ampl_loss: 0.0139 - outputs_accuracy: 0.9266 - val_loss: 0.6121 - val_outputs_loss: 0.2331 - val_rri_loss: 0.0527 - val_ampl_loss: 0.0138 - val_outputs_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.5676 - outputs_loss: 0.1890 - rri_loss: 0.0528 - ampl_loss: 0.0152 - outputs_accuracy: 0.9288\n",
      "Epoch 41: val_outputs_accuracy did not improve from 0.92062\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.5673 - outputs_loss: 0.1887 - rri_loss: 0.0528 - ampl_loss: 0.0152 - outputs_accuracy: 0.9290 - val_loss: 0.5897 - val_outputs_loss: 0.2143 - val_rri_loss: 0.0526 - val_ampl_loss: 0.0145 - val_outputs_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.5654 - outputs_loss: 0.1919 - rri_loss: 0.0524 - ampl_loss: 0.0147 - outputs_accuracy: 0.9278\n",
      "Epoch 42: val_outputs_accuracy did not improve from 0.92062\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.5654 - outputs_loss: 0.1919 - rri_loss: 0.0524 - ampl_loss: 0.0147 - outputs_accuracy: 0.9279 - val_loss: 0.5909 - val_outputs_loss: 0.2197 - val_rri_loss: 0.0527 - val_ampl_loss: 0.0141 - val_outputs_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.5654 - outputs_loss: 0.1966 - rri_loss: 0.0527 - ampl_loss: 0.0139 - outputs_accuracy: 0.9270\n",
      "Epoch 43: val_outputs_accuracy did not improve from 0.92062\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.5661 - outputs_loss: 0.1972 - rri_loss: 0.0527 - ampl_loss: 0.0139 - outputs_accuracy: 0.9267 - val_loss: 0.6167 - val_outputs_loss: 0.2446 - val_rri_loss: 0.0530 - val_ampl_loss: 0.0190 - val_outputs_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.5619 - outputs_loss: 0.1966 - rri_loss: 0.0525 - ampl_loss: 0.0146 - outputs_accuracy: 0.9291\n",
      "Epoch 44: val_outputs_accuracy did not improve from 0.92062\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.5616 - outputs_loss: 0.1963 - rri_loss: 0.0524 - ampl_loss: 0.0146 - outputs_accuracy: 0.9291 - val_loss: 0.5918 - val_outputs_loss: 0.2254 - val_rri_loss: 0.0551 - val_ampl_loss: 0.0153 - val_outputs_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.5527 - outputs_loss: 0.1911 - rri_loss: 0.0526 - ampl_loss: 0.0150 - outputs_accuracy: 0.9291\n",
      "Epoch 45: val_outputs_accuracy did not improve from 0.92062\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.5526 - outputs_loss: 0.1910 - rri_loss: 0.0526 - ampl_loss: 0.0150 - outputs_accuracy: 0.9291 - val_loss: 0.6195 - val_outputs_loss: 0.2602 - val_rri_loss: 0.0526 - val_ampl_loss: 0.0149 - val_outputs_accuracy: 0.8987 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.5528 - outputs_loss: 0.1962 - rri_loss: 0.0526 - ampl_loss: 0.0140 - outputs_accuracy: 0.9268\n",
      "Epoch 46: val_outputs_accuracy did not improve from 0.92062\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.5522 - outputs_loss: 0.1957 - rri_loss: 0.0526 - ampl_loss: 0.0140 - outputs_accuracy: 0.9270 - val_loss: 0.5704 - val_outputs_loss: 0.2156 - val_rri_loss: 0.0530 - val_ampl_loss: 0.0141 - val_outputs_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.5407 - outputs_loss: 0.1882 - rri_loss: 0.0524 - ampl_loss: 0.0144 - outputs_accuracy: 0.9300\n",
      "Epoch 47: val_outputs_accuracy did not improve from 0.92062\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.5403 - outputs_loss: 0.1879 - rri_loss: 0.0524 - ampl_loss: 0.0144 - outputs_accuracy: 0.9302 - val_loss: 0.5930 - val_outputs_loss: 0.2418 - val_rri_loss: 0.0528 - val_ampl_loss: 0.0148 - val_outputs_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.5373 - outputs_loss: 0.1891 - rri_loss: 0.0524 - ampl_loss: 0.0142 - outputs_accuracy: 0.9300\n",
      "Epoch 48: val_outputs_accuracy did not improve from 0.92062\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.5373 - outputs_loss: 0.1891 - rri_loss: 0.0524 - ampl_loss: 0.0142 - outputs_accuracy: 0.9300 - val_loss: 0.5817 - val_outputs_loss: 0.2354 - val_rri_loss: 0.0525 - val_ampl_loss: 0.0145 - val_outputs_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.5371 - outputs_loss: 0.1922 - rri_loss: 0.0523 - ampl_loss: 0.0149 - outputs_accuracy: 0.9278\n",
      "Epoch 49: val_outputs_accuracy did not improve from 0.92062\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.5365 - outputs_loss: 0.1916 - rri_loss: 0.0523 - ampl_loss: 0.0149 - outputs_accuracy: 0.9281 - val_loss: 0.5825 - val_outputs_loss: 0.2405 - val_rri_loss: 0.0524 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.5235 - outputs_loss: 0.1833 - rri_loss: 0.0525 - ampl_loss: 0.0140 - outputs_accuracy: 0.9326\n",
      "Epoch 50: val_outputs_accuracy improved from 0.92062 to 0.92401, saving model to /content/drive/MyDrive/Final Result/Final_performance/BAFNet/model/min_mean_1.hdf5\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.5235 - outputs_loss: 0.1833 - rri_loss: 0.0525 - ampl_loss: 0.0140 - outputs_accuracy: 0.9326 - val_loss: 0.5518 - val_outputs_loss: 0.2127 - val_rri_loss: 0.0526 - val_ampl_loss: 0.0149 - val_outputs_accuracy: 0.9240 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.5178 - outputs_loss: 0.1816 - rri_loss: 0.0526 - ampl_loss: 0.0141 - outputs_accuracy: 0.9320\n",
      "Epoch 51: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.5187 - outputs_loss: 0.1825 - rri_loss: 0.0526 - ampl_loss: 0.0141 - outputs_accuracy: 0.9318 - val_loss: 0.5565 - val_outputs_loss: 0.2222 - val_rri_loss: 0.0528 - val_ampl_loss: 0.0140 - val_outputs_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 0.5112 - outputs_loss: 0.1788 - rri_loss: 0.0526 - ampl_loss: 0.0140 - outputs_accuracy: 0.9327\n",
      "Epoch 52: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.5113 - outputs_loss: 0.1789 - rri_loss: 0.0526 - ampl_loss: 0.0140 - outputs_accuracy: 0.9327 - val_loss: 0.5526 - val_outputs_loss: 0.2219 - val_rri_loss: 0.0527 - val_ampl_loss: 0.0145 - val_outputs_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.5088 - outputs_loss: 0.1803 - rri_loss: 0.0524 - ampl_loss: 0.0143 - outputs_accuracy: 0.9342\n",
      "Epoch 53: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.5090 - outputs_loss: 0.1806 - rri_loss: 0.0524 - ampl_loss: 0.0143 - outputs_accuracy: 0.9341 - val_loss: 0.5390 - val_outputs_loss: 0.2119 - val_rri_loss: 0.0529 - val_ampl_loss: 0.0145 - val_outputs_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.4933 - outputs_loss: 0.1688 - rri_loss: 0.0523 - ampl_loss: 0.0144 - outputs_accuracy: 0.9396\n",
      "Epoch 54: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4939 - outputs_loss: 0.1694 - rri_loss: 0.0523 - ampl_loss: 0.0144 - outputs_accuracy: 0.9395 - val_loss: 0.5759 - val_outputs_loss: 0.2533 - val_rri_loss: 0.0529 - val_ampl_loss: 0.0137 - val_outputs_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.4946 - outputs_loss: 0.1735 - rri_loss: 0.0525 - ampl_loss: 0.0145 - outputs_accuracy: 0.9356\n",
      "Epoch 55: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4948 - outputs_loss: 0.1738 - rri_loss: 0.0525 - ampl_loss: 0.0145 - outputs_accuracy: 0.9356 - val_loss: 0.5435 - val_outputs_loss: 0.2239 - val_rri_loss: 0.0524 - val_ampl_loss: 0.0150 - val_outputs_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.4881 - outputs_loss: 0.1712 - rri_loss: 0.0524 - ampl_loss: 0.0141 - outputs_accuracy: 0.9374\n",
      "Epoch 56: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4880 - outputs_loss: 0.1711 - rri_loss: 0.0524 - ampl_loss: 0.0141 - outputs_accuracy: 0.9374 - val_loss: 0.5316 - val_outputs_loss: 0.2158 - val_rri_loss: 0.0524 - val_ampl_loss: 0.0149 - val_outputs_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.4826 - outputs_loss: 0.1694 - rri_loss: 0.0523 - ampl_loss: 0.0144 - outputs_accuracy: 0.9405\n",
      "Epoch 57: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4825 - outputs_loss: 0.1692 - rri_loss: 0.0523 - ampl_loss: 0.0144 - outputs_accuracy: 0.9405 - val_loss: 0.5436 - val_outputs_loss: 0.2316 - val_rri_loss: 0.0530 - val_ampl_loss: 0.0145 - val_outputs_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.4832 - outputs_loss: 0.1740 - rri_loss: 0.0523 - ampl_loss: 0.0142 - outputs_accuracy: 0.9356\n",
      "Epoch 58: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4833 - outputs_loss: 0.1741 - rri_loss: 0.0523 - ampl_loss: 0.0142 - outputs_accuracy: 0.9356 - val_loss: 0.5445 - val_outputs_loss: 0.2376 - val_rri_loss: 0.0524 - val_ampl_loss: 0.0137 - val_outputs_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.4773 - outputs_loss: 0.1717 - rri_loss: 0.0523 - ampl_loss: 0.0141 - outputs_accuracy: 0.9363\n",
      "Epoch 59: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.4772 - outputs_loss: 0.1716 - rri_loss: 0.0523 - ampl_loss: 0.0141 - outputs_accuracy: 0.9363 - val_loss: 0.5332 - val_outputs_loss: 0.2298 - val_rri_loss: 0.0524 - val_ampl_loss: 0.0138 - val_outputs_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.4725 - outputs_loss: 0.1708 - rri_loss: 0.0523 - ampl_loss: 0.0140 - outputs_accuracy: 0.9380\n",
      "Epoch 60: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4724 - outputs_loss: 0.1706 - rri_loss: 0.0523 - ampl_loss: 0.0140 - outputs_accuracy: 0.9382 - val_loss: 0.5300 - val_outputs_loss: 0.2288 - val_rri_loss: 0.0531 - val_ampl_loss: 0.0142 - val_outputs_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.4604 - outputs_loss: 0.1616 - rri_loss: 0.0523 - ampl_loss: 0.0145 - outputs_accuracy: 0.9428\n",
      "Epoch 61: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4601 - outputs_loss: 0.1613 - rri_loss: 0.0523 - ampl_loss: 0.0145 - outputs_accuracy: 0.9429 - val_loss: 0.5406 - val_outputs_loss: 0.2440 - val_rri_loss: 0.0524 - val_ampl_loss: 0.0142 - val_outputs_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - ETA: 0s - loss: 0.4568 - outputs_loss: 0.1623 - rri_loss: 0.0523 - ampl_loss: 0.0140 - outputs_accuracy: 0.9411\n",
      "Epoch 62: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.4568 - outputs_loss: 0.1623 - rri_loss: 0.0523 - ampl_loss: 0.0140 - outputs_accuracy: 0.9411 - val_loss: 0.5248 - val_outputs_loss: 0.2317 - val_rri_loss: 0.0525 - val_ampl_loss: 0.0141 - val_outputs_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.4506 - outputs_loss: 0.1593 - rri_loss: 0.0523 - ampl_loss: 0.0143 - outputs_accuracy: 0.9397\n",
      "Epoch 63: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4507 - outputs_loss: 0.1594 - rri_loss: 0.0523 - ampl_loss: 0.0143 - outputs_accuracy: 0.9396 - val_loss: 0.5095 - val_outputs_loss: 0.2202 - val_rri_loss: 0.0525 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9210 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.4484 - outputs_loss: 0.1605 - rri_loss: 0.0523 - ampl_loss: 0.0142 - outputs_accuracy: 0.9413\n",
      "Epoch 64: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.4487 - outputs_loss: 0.1607 - rri_loss: 0.0523 - ampl_loss: 0.0142 - outputs_accuracy: 0.9411 - val_loss: 0.5193 - val_outputs_loss: 0.2332 - val_rri_loss: 0.0524 - val_ampl_loss: 0.0142 - val_outputs_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.4381 - outputs_loss: 0.1537 - rri_loss: 0.0522 - ampl_loss: 0.0142 - outputs_accuracy: 0.9461\n",
      "Epoch 65: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.4381 - outputs_loss: 0.1537 - rri_loss: 0.0522 - ampl_loss: 0.0142 - outputs_accuracy: 0.9461 - val_loss: 0.5122 - val_outputs_loss: 0.2287 - val_rri_loss: 0.0528 - val_ampl_loss: 0.0147 - val_outputs_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.4402 - outputs_loss: 0.1588 - rri_loss: 0.0524 - ampl_loss: 0.0147 - outputs_accuracy: 0.9440\n",
      "Epoch 66: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4407 - outputs_loss: 0.1593 - rri_loss: 0.0524 - ampl_loss: 0.0147 - outputs_accuracy: 0.9437 - val_loss: 0.5097 - val_outputs_loss: 0.2289 - val_rri_loss: 0.0524 - val_ampl_loss: 0.0155 - val_outputs_accuracy: 0.9188 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 0.4276 - outputs_loss: 0.1497 - rri_loss: 0.0522 - ampl_loss: 0.0144 - outputs_accuracy: 0.9466\n",
      "Epoch 67: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4273 - outputs_loss: 0.1494 - rri_loss: 0.0522 - ampl_loss: 0.0144 - outputs_accuracy: 0.9465 - val_loss: 0.5136 - val_outputs_loss: 0.2377 - val_rri_loss: 0.0524 - val_ampl_loss: 0.0140 - val_outputs_accuracy: 0.9186 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.4255 - outputs_loss: 0.1515 - rri_loss: 0.0523 - ampl_loss: 0.0140 - outputs_accuracy: 0.9440\n",
      "Epoch 68: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4262 - outputs_loss: 0.1521 - rri_loss: 0.0523 - ampl_loss: 0.0140 - outputs_accuracy: 0.9439 - val_loss: 0.5209 - val_outputs_loss: 0.2485 - val_rri_loss: 0.0524 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.4182 - outputs_loss: 0.1474 - rri_loss: 0.0523 - ampl_loss: 0.0142 - outputs_accuracy: 0.9477\n",
      "Epoch 69: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4192 - outputs_loss: 0.1484 - rri_loss: 0.0523 - ampl_loss: 0.0143 - outputs_accuracy: 0.9475 - val_loss: 0.4931 - val_outputs_loss: 0.2230 - val_rri_loss: 0.0527 - val_ampl_loss: 0.0153 - val_outputs_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.4155 - outputs_loss: 0.1481 - rri_loss: 0.0526 - ampl_loss: 0.0141 - outputs_accuracy: 0.9479\n",
      "Epoch 70: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.4154 - outputs_loss: 0.1480 - rri_loss: 0.0526 - ampl_loss: 0.0141 - outputs_accuracy: 0.9479 - val_loss: 0.4931 - val_outputs_loss: 0.2276 - val_rri_loss: 0.0524 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.4007 - outputs_loss: 0.1369 - rri_loss: 0.0521 - ampl_loss: 0.0140 - outputs_accuracy: 0.9531\n",
      "Epoch 71: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4006 - outputs_loss: 0.1368 - rri_loss: 0.0521 - ampl_loss: 0.0141 - outputs_accuracy: 0.9532 - val_loss: 0.4992 - val_outputs_loss: 0.2366 - val_rri_loss: 0.0523 - val_ampl_loss: 0.0145 - val_outputs_accuracy: 0.9184 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 0.3852 - outputs_loss: 0.1239 - rri_loss: 0.0520 - ampl_loss: 0.0137 - outputs_accuracy: 0.9582\n",
      "Epoch 72: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3845 - outputs_loss: 0.1232 - rri_loss: 0.0520 - ampl_loss: 0.0137 - outputs_accuracy: 0.9584 - val_loss: 0.4950 - val_outputs_loss: 0.2334 - val_rri_loss: 0.0522 - val_ampl_loss: 0.0138 - val_outputs_accuracy: 0.9186 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.3757 - outputs_loss: 0.1149 - rri_loss: 0.0519 - ampl_loss: 0.0136 - outputs_accuracy: 0.9632\n",
      "Epoch 73: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3755 - outputs_loss: 0.1147 - rri_loss: 0.0519 - ampl_loss: 0.0136 - outputs_accuracy: 0.9632 - val_loss: 0.4996 - val_outputs_loss: 0.2384 - val_rri_loss: 0.0522 - val_ampl_loss: 0.0138 - val_outputs_accuracy: 0.9186 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.3728 - outputs_loss: 0.1123 - rri_loss: 0.0519 - ampl_loss: 0.0136 - outputs_accuracy: 0.9645\n",
      "Epoch 74: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3727 - outputs_loss: 0.1122 - rri_loss: 0.0519 - ampl_loss: 0.0136 - outputs_accuracy: 0.9645 - val_loss: 0.5164 - val_outputs_loss: 0.2556 - val_rri_loss: 0.0522 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9154 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.3725 - outputs_loss: 0.1124 - rri_loss: 0.0519 - ampl_loss: 0.0137 - outputs_accuracy: 0.9647\n",
      "Epoch 75: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3723 - outputs_loss: 0.1122 - rri_loss: 0.0519 - ampl_loss: 0.0137 - outputs_accuracy: 0.9648 - val_loss: 0.5027 - val_outputs_loss: 0.2423 - val_rri_loss: 0.0522 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9154 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.3719 - outputs_loss: 0.1122 - rri_loss: 0.0519 - ampl_loss: 0.0137 - outputs_accuracy: 0.9651\n",
      "Epoch 76: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3725 - outputs_loss: 0.1129 - rri_loss: 0.0519 - ampl_loss: 0.0137 - outputs_accuracy: 0.9647 - val_loss: 0.5196 - val_outputs_loss: 0.2595 - val_rri_loss: 0.0522 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9101 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 0.3721 - outputs_loss: 0.1127 - rri_loss: 0.0519 - ampl_loss: 0.0137 - outputs_accuracy: 0.9635\n",
      "Epoch 77: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3713 - outputs_loss: 0.1120 - rri_loss: 0.0519 - ampl_loss: 0.0137 - outputs_accuracy: 0.9637 - val_loss: 0.5072 - val_outputs_loss: 0.2475 - val_rri_loss: 0.0522 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9148 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.3690 - outputs_loss: 0.1101 - rri_loss: 0.0519 - ampl_loss: 0.0137 - outputs_accuracy: 0.9649\n",
      "Epoch 78: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3692 - outputs_loss: 0.1103 - rri_loss: 0.0519 - ampl_loss: 0.0137 - outputs_accuracy: 0.9649 - val_loss: 0.5021 - val_outputs_loss: 0.2429 - val_rri_loss: 0.0522 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9172 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.3670 - outputs_loss: 0.1085 - rri_loss: 0.0519 - ampl_loss: 0.0137 - outputs_accuracy: 0.9670\n",
      "Epoch 79: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3667 - outputs_loss: 0.1082 - rri_loss: 0.0519 - ampl_loss: 0.0137 - outputs_accuracy: 0.9671 - val_loss: 0.5035 - val_outputs_loss: 0.2447 - val_rri_loss: 0.0522 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9170 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.3652 - outputs_loss: 0.1072 - rri_loss: 0.0519 - ampl_loss: 0.0137 - outputs_accuracy: 0.9661\n",
      "Epoch 80: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3650 - outputs_loss: 0.1070 - rri_loss: 0.0519 - ampl_loss: 0.0137 - outputs_accuracy: 0.9662 - val_loss: 0.5060 - val_outputs_loss: 0.2476 - val_rri_loss: 0.0522 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9158 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.3636 - outputs_loss: 0.1061 - rri_loss: 0.0519 - ampl_loss: 0.0137 - outputs_accuracy: 0.9663\n",
      "Epoch 81: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3636 - outputs_loss: 0.1061 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9663 - val_loss: 0.5075 - val_outputs_loss: 0.2495 - val_rri_loss: 0.0522 - val_ampl_loss: 0.0140 - val_outputs_accuracy: 0.9152 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.3593 - outputs_loss: 0.1020 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9686\n",
      "Epoch 82: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3597 - outputs_loss: 0.1025 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9683 - val_loss: 0.5059 - val_outputs_loss: 0.2481 - val_rri_loss: 0.0522 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9146 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.3588 - outputs_loss: 0.1016 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9690\n",
      "Epoch 83: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3589 - outputs_loss: 0.1017 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9689 - val_loss: 0.5063 - val_outputs_loss: 0.2485 - val_rri_loss: 0.0522 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9150 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.3587 - outputs_loss: 0.1016 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9694\n",
      "Epoch 84: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3586 - outputs_loss: 0.1015 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9694 - val_loss: 0.5071 - val_outputs_loss: 0.2495 - val_rri_loss: 0.0522 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9152 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.3585 - outputs_loss: 0.1014 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9694\n",
      "Epoch 85: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3586 - outputs_loss: 0.1015 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9695 - val_loss: 0.5077 - val_outputs_loss: 0.2501 - val_rri_loss: 0.0521 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9156 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.3580 - outputs_loss: 0.1010 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9693\n",
      "Epoch 86: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3582 - outputs_loss: 0.1011 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9693 - val_loss: 0.5084 - val_outputs_loss: 0.2509 - val_rri_loss: 0.0521 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9148 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.3580 - outputs_loss: 0.1011 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9687\n",
      "Epoch 87: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3582 - outputs_loss: 0.1013 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9685 - val_loss: 0.5084 - val_outputs_loss: 0.2509 - val_rri_loss: 0.0521 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9148 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.3576 - outputs_loss: 0.1008 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9698\n",
      "Epoch 88: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3578 - outputs_loss: 0.1009 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9696 - val_loss: 0.5092 - val_outputs_loss: 0.2517 - val_rri_loss: 0.0521 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9160 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.3572 - outputs_loss: 0.1004 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9698\n",
      "Epoch 89: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3575 - outputs_loss: 0.1007 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9697 - val_loss: 0.5087 - val_outputs_loss: 0.2513 - val_rri_loss: 0.0521 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9158 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.3572 - outputs_loss: 0.1004 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9699\n",
      "Epoch 90: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3576 - outputs_loss: 0.1008 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9698 - val_loss: 0.5092 - val_outputs_loss: 0.2519 - val_rri_loss: 0.0521 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9158 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 0.3570 - outputs_loss: 0.1004 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9696\n",
      "Epoch 91: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3573 - outputs_loss: 0.1007 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9695 - val_loss: 0.5100 - val_outputs_loss: 0.2528 - val_rri_loss: 0.0521 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9156 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.3567 - outputs_loss: 0.1000 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9702\n",
      "Epoch 92: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3567 - outputs_loss: 0.1001 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9703 - val_loss: 0.5102 - val_outputs_loss: 0.2529 - val_rri_loss: 0.0521 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9150 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.3569 - outputs_loss: 0.1002 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9698\n",
      "Epoch 93: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3567 - outputs_loss: 0.1001 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9698 - val_loss: 0.5102 - val_outputs_loss: 0.2530 - val_rri_loss: 0.0521 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9150 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.3564 - outputs_loss: 0.0998 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9699\n",
      "Epoch 94: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3566 - outputs_loss: 0.1000 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9697 - val_loss: 0.5101 - val_outputs_loss: 0.2529 - val_rri_loss: 0.0521 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9148 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "90/92 [============================>.] - ETA: 0s - loss: 0.3564 - outputs_loss: 0.0999 - rri_loss: 0.0517 - ampl_loss: 0.0137 - outputs_accuracy: 0.9702\n",
      "Epoch 95: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3566 - outputs_loss: 0.1000 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9700 - val_loss: 0.5102 - val_outputs_loss: 0.2530 - val_rri_loss: 0.0521 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9148 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.3566 - outputs_loss: 0.1000 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9700\n",
      "Epoch 96: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3566 - outputs_loss: 0.1000 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9699 - val_loss: 0.5102 - val_outputs_loss: 0.2530 - val_rri_loss: 0.0521 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9154 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.3567 - outputs_loss: 0.1001 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9700\n",
      "Epoch 97: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3566 - outputs_loss: 0.1000 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9700 - val_loss: 0.5103 - val_outputs_loss: 0.2531 - val_rri_loss: 0.0521 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9150 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.3567 - outputs_loss: 0.1001 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9699\n",
      "Epoch 98: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3566 - outputs_loss: 0.1000 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9699 - val_loss: 0.5104 - val_outputs_loss: 0.2533 - val_rri_loss: 0.0521 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9150 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.3567 - outputs_loss: 0.1001 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9699\n",
      "Epoch 99: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3566 - outputs_loss: 0.1000 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9698 - val_loss: 0.5104 - val_outputs_loss: 0.2532 - val_rri_loss: 0.0521 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9152 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "91/92 [============================>.] - ETA: 0s - loss: 0.3567 - outputs_loss: 0.1001 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9697\n",
      "Epoch 100: val_outputs_accuracy did not improve from 0.92401\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3565 - outputs_loss: 0.0999 - rri_loss: 0.0518 - ampl_loss: 0.0137 - outputs_accuracy: 0.9697 - val_loss: 0.5103 - val_outputs_loss: 0.2532 - val_rri_loss: 0.0521 - val_ampl_loss: 0.0139 - val_outputs_accuracy: 0.9150 - lr: 1.0000e-06\n",
      "time_train: 259.92429780960083\n",
      "{'loss': [3.6325740814208984, 1.33046293258667, 1.2862646579742432, 1.2496346235275269, 1.1912713050842285, 1.0807894468307495, 0.9386515021324158, 0.8719504475593567, 0.8386773467063904, 0.8102652430534363, 0.8002346754074097, 0.7704386711120605, 0.7689805030822754, 0.7512590289115906, 0.7422467470169067, 0.7404760122299194, 0.723876416683197, 0.7264614701271057, 0.7089551687240601, 0.703835129737854, 0.699627161026001, 0.694086492061615, 0.6806015372276306, 0.6746201515197754, 0.6646040081977844, 0.6657562851905823, 0.6727722883224487, 0.6559313535690308, 0.6380457878112793, 0.6339074969291687, 0.6399613618850708, 0.6250567436218262, 0.6154643893241882, 0.624741792678833, 0.6100139021873474, 0.6018369793891907, 0.5937865376472473, 0.5945319533348083, 0.5905595421791077, 0.5787187814712524, 0.5672964453697205, 0.5653863549232483, 0.5660932064056396, 0.5615894794464111, 0.5525953769683838, 0.5522196888923645, 0.5403426289558411, 0.5373404622077942, 0.5364838242530823, 0.523489236831665, 0.5187200307846069, 0.5113142132759094, 0.5090413689613342, 0.4939039647579193, 0.49483391642570496, 0.4880019426345825, 0.48246169090270996, 0.4833424687385559, 0.4771755039691925, 0.47238633036613464, 0.46009132266044617, 0.456842303276062, 0.450730562210083, 0.44867002964019775, 0.43806010484695435, 0.440746933221817, 0.4273308217525482, 0.4261837303638458, 0.4192250669002533, 0.41539716720581055, 0.4005642533302307, 0.3844873309135437, 0.37549880146980286, 0.3727138936519623, 0.37226465344429016, 0.37254881858825684, 0.3713216781616211, 0.36916446685791016, 0.3667120635509491, 0.3649839460849762, 0.36364611983299255, 0.3597082197666168, 0.3588891327381134, 0.3585788607597351, 0.3585720956325531, 0.35815292596817017, 0.3582281172275543, 0.3577878773212433, 0.3575323224067688, 0.3575722575187683, 0.35733842849731445, 0.35669901967048645, 0.35667917132377625, 0.356638640165329, 0.3566264808177948, 0.35661301016807556, 0.35658028721809387, 0.35657066106796265, 0.3565555810928345, 0.3564991056919098], 'outputs_loss': [0.6776804327964783, 0.6602781414985657, 0.6580287218093872, 0.6497312784194946, 0.608232319355011, 0.49861666560173035, 0.3657025396823883, 0.3154871165752411, 0.29012513160705566, 0.2733890116214752, 0.2699018120765686, 0.2492712438106537, 0.25507381558418274, 0.24441535770893097, 0.240592360496521, 0.2448863536119461, 0.2345939725637436, 0.24293741583824158, 0.23108026385307312, 0.23152703046798706, 0.231257826089859, 0.23193354904651642, 0.2221604585647583, 0.22120177745819092, 0.21537908911705017, 0.22268688678741455, 0.23332762718200684, 0.2216140329837799, 0.2076265513896942, 0.2082018405199051, 0.21954582631587982, 0.2086666375398636, 0.2038021832704544, 0.20873761177062988, 0.20690731704235077, 0.2039528340101242, 0.1995699256658554, 0.20436128973960876, 0.20467300713062286, 0.19742710888385773, 0.18871022760868073, 0.19186988472938538, 0.19721685349941254, 0.19630354642868042, 0.19098779559135437, 0.1956649273633957, 0.18787170946598053, 0.1891399323940277, 0.19164492189884186, 0.18334566056728363, 0.18253746628761292, 0.17890912294387817, 0.18061047792434692, 0.16939523816108704, 0.17381559312343597, 0.17112918198108673, 0.16920436918735504, 0.17413674294948578, 0.17158785462379456, 0.17064929008483887, 0.16127240657806396, 0.16226278245449066, 0.1593668907880783, 0.16071739792823792, 0.1537129282951355, 0.15931016206741333, 0.14942339062690735, 0.152142733335495, 0.14839133620262146, 0.14804154634475708, 0.13678887486457825, 0.12318950891494751, 0.11466875672340393, 0.11222820729017258, 0.11215398460626602, 0.1128547191619873, 0.11196453869342804, 0.11027858406305313, 0.10822070389986038, 0.10695954412221909, 0.10609515756368637, 0.10247381031513214, 0.10172654688358307, 0.10147068649530411, 0.10150939226150513, 0.10114605724811554, 0.10128094255924225, 0.10090506821870804, 0.10071350634098053, 0.10081590712070465, 0.10066170245409012, 0.10006332397460938, 0.10005307197570801, 0.10002009570598602, 0.10001670569181442, 0.1000111997127533, 0.09998756647109985, 0.09998668730258942, 0.09998088330030441, 0.09993507713079453], 'rri_loss': [0.6815205812454224, 0.05674260854721069, 0.055217910557985306, 0.05413361266255379, 0.055779386311769485, 0.06424016505479813, 0.059535153210163116, 0.056251898407936096, 0.05684913322329521, 0.05576927587389946, 0.05681905150413513, 0.05504629388451576, 0.05559000372886658, 0.054487571120262146, 0.05414951965212822, 0.05452561378479004, 0.0540257953107357, 0.053921137005090714, 0.05380610376596451, 0.05397628992795944, 0.05439506843686104, 0.05365373194217682, 0.05402735620737076, 0.053996115922927856, 0.05336744338274002, 0.05337323248386383, 0.05368373915553093, 0.05328025296330452, 0.05301866680383682, 0.05333529785275459, 0.053245365619659424, 0.05297613516449928, 0.05306167155504227, 0.05368635430932045, 0.05300626903772354, 0.05252045765519142, 0.0526154600083828, 0.05278931185603142, 0.052567269653081894, 0.05260094255208969, 0.05281461775302887, 0.05238078534603119, 0.052681129425764084, 0.052430856972932816, 0.052593130618333817, 0.05261342599987984, 0.05241289734840393, 0.052350420504808426, 0.052316613495349884, 0.05247967317700386, 0.052617330104112625, 0.05263109505176544, 0.052428245544433594, 0.05226606875658035, 0.05250003561377525, 0.05236000567674637, 0.0523049533367157, 0.05227189138531685, 0.05234701931476593, 0.05228711664676666, 0.052333783358335495, 0.052287518978118896, 0.05233420059084892, 0.052341278642416, 0.052163247019052505, 0.052392393350601196, 0.052241455763578415, 0.05231142416596413, 0.05227503180503845, 0.052568286657333374, 0.0521438904106617, 0.0519782155752182, 0.05191676691174507, 0.05190042778849602, 0.051894355565309525, 0.051897261291742325, 0.05189047381281853, 0.05189012363553047, 0.05189426988363266, 0.05186019837856293, 0.05183941870927811, 0.05183180049061775, 0.05181807652115822, 0.05181329697370529, 0.05180961266160011, 0.05181332677602768, 0.0518062524497509, 0.05180535465478897, 0.05180482938885689, 0.0518062599003315, 0.051800988614559174, 0.051795970648527145, 0.05179530754685402, 0.05179538205265999, 0.05179504305124283, 0.051795076578855515, 0.05179485306143761, 0.05179489031434059, 0.05179480090737343, 0.05179433897137642], 'ampl_loss': [1.6753865480422974, 0.06298770010471344, 0.048156071454286575, 0.038719795644283295, 0.03305089846253395, 0.032471511512994766, 0.03466496244072914, 0.028311748057603836, 0.02689737267792225, 0.023226287215948105, 0.02164287306368351, 0.020403021946549416, 0.018518945202231407, 0.01834265887737274, 0.019085627049207687, 0.018118930980563164, 0.01739484816789627, 0.016992462798953056, 0.0164701659232378, 0.015826065093278885, 0.0161117035895586, 0.01539554726332426, 0.016039874404668808, 0.015600107610225677, 0.016785161569714546, 0.015159633941948414, 0.0152096813544631, 0.014817388728260994, 0.015584715642035007, 0.015069495886564255, 0.014318574219942093, 0.014955894090235233, 0.014566386118531227, 0.022502079606056213, 0.014573127031326294, 0.013964006677269936, 0.014439921826124191, 0.014132305979728699, 0.014309714548289776, 0.013948614709079266, 0.015235240571200848, 0.014707854948937893, 0.01386295910924673, 0.014618903398513794, 0.014992264099419117, 0.014029767364263535, 0.014364049769937992, 0.014179128222167492, 0.01491623092442751, 0.014042213559150696, 0.01405521109700203, 0.01402144506573677, 0.01428042259067297, 0.014385455287992954, 0.014515257440507412, 0.014100436121225357, 0.014376518316566944, 0.014191914349794388, 0.014101442880928516, 0.013978849165141582, 0.014532659202814102, 0.014011493884027004, 0.014284842647612095, 0.014246286824345589, 0.014150221832096577, 0.014652978628873825, 0.014407604932785034, 0.014001826755702496, 0.014254647307097912, 0.014142945408821106, 0.014051403850317001, 0.013659908436238766, 0.013600905425846577, 0.01364566944539547, 0.01367288175970316, 0.013657639734447002, 0.01371783297508955, 0.013673001900315285, 0.013686086051166058, 0.01370336301624775, 0.013708348385989666, 0.01367262378334999, 0.013656102120876312, 0.013657054863870144, 0.013666498474776745, 0.013662833720445633, 0.013668710365891457, 0.013665749691426754, 0.013663793914020061, 0.013667607679963112, 0.013665842823684216, 0.013666407205164433, 0.013664632104337215, 0.013665184378623962, 0.013664761558175087, 0.013664700090885162, 0.013664879836142063, 0.013665247708559036, 0.01366525050252676, 0.013664957135915756], 'outputs_accuracy': [0.5777416825294495, 0.6126164793968201, 0.6126164793968201, 0.6126164793968201, 0.6623643040657043, 0.7642533779144287, 0.844602108001709, 0.8711000680923462, 0.8861441016197205, 0.8944354057312012, 0.8954611420631409, 0.9025557637214661, 0.9017009735107422, 0.9072570204734802, 0.9077699184417725, 0.907513439655304, 0.9104197025299072, 0.9061458110809326, 0.913838803768158, 0.9102487564086914, 0.9122146964073181, 0.9102487564086914, 0.9167450070381165, 0.9164885878562927, 0.9176852703094482, 0.9158902764320374, 0.9119582772254944, 0.9166595339775085, 0.9224720001220703, 0.9214462637901306, 0.9178562164306641, 0.9210188984870911, 0.9243525266647339, 0.9222155809402466, 0.9223010540008545, 0.921788215637207, 0.9267458915710449, 0.9236686825752258, 0.9223010540008545, 0.9265749454498291, 0.9289683103561401, 0.9278571009635925, 0.9267458915710449, 0.929139256477356, 0.929053783416748, 0.9270023107528687, 0.9301649928092957, 0.9299939870834351, 0.9281135201454163, 0.9325583577156067, 0.9317890405654907, 0.9327293038368225, 0.9340969324111938, 0.9394820332527161, 0.9355500340461731, 0.9374305605888367, 0.940507709980011, 0.9355500340461731, 0.9363193511962891, 0.9381998181343079, 0.942901074886322, 0.9411060810089111, 0.939567506313324, 0.9411060810089111, 0.946063756942749, 0.943670392036438, 0.9464911818504333, 0.9439268112182617, 0.9475168585777283, 0.9479442834854126, 0.9531583786010742, 0.9583725333213806, 0.9631592631340027, 0.964526891708374, 0.9647833108901978, 0.9646978378295898, 0.9636721014976501, 0.9648687839508057, 0.9670912027359009, 0.9661509394645691, 0.9663218855857849, 0.9682878851890564, 0.9688862562179565, 0.969399094581604, 0.9694845676422119, 0.9693136215209961, 0.9685443043708801, 0.9695700407028198, 0.9697409868240356, 0.9698264598846436, 0.9694845676422119, 0.9702538847923279, 0.9698264598846436, 0.9697409868240356, 0.9699974060058594, 0.9699119329452515, 0.9699974060058594, 0.9699119329452515, 0.9698264598846436, 0.9697409868240356], 'val_loss': [1.3601576089859009, 1.3045626878738403, 1.266204595565796, 1.2274867296218872, 1.139039158821106, 1.019911289215088, 0.884108304977417, 0.8722098469734192, 0.818176805973053, 0.8082480430603027, 0.7930507063865662, 0.7726559042930603, 0.7659810781478882, 0.7516331076622009, 0.7905972599983215, 0.776347815990448, 0.7344022393226624, 0.7400543689727783, 0.7099287509918213, 0.7287265658378601, 0.697213888168335, 0.69232177734375, 0.7429770231246948, 0.6819310784339905, 0.6903751492500305, 0.7406697869300842, 0.6643194556236267, 0.6607477068901062, 0.6521123647689819, 0.6552246809005737, 0.6396569609642029, 0.6523426175117493, 0.6410481333732605, 0.6421478390693665, 0.6223140358924866, 0.6143771409988403, 0.6212478280067444, 0.6077362298965454, 0.6260433197021484, 0.6120519638061523, 0.5896503329277039, 0.5908651351928711, 0.6166574358940125, 0.5917571187019348, 0.6195248365402222, 0.5704493522644043, 0.5929584503173828, 0.5817437767982483, 0.5825172662734985, 0.5517914295196533, 0.5564584136009216, 0.552608072757721, 0.5389829874038696, 0.57589191198349, 0.5434755086898804, 0.5316037535667419, 0.543600857257843, 0.5444777011871338, 0.533211886882782, 0.5300149321556091, 0.5405657887458801, 0.524842381477356, 0.5095028877258301, 0.5193020105361938, 0.5121950507164001, 0.5097327828407288, 0.5135722160339355, 0.5208590030670166, 0.4931025207042694, 0.4930534362792969, 0.49920928478240967, 0.4949815273284912, 0.49963411688804626, 0.5164497494697571, 0.5027288794517517, 0.5195618271827698, 0.5072059631347656, 0.5020843744277954, 0.5034514665603638, 0.5059937238693237, 0.507463276386261, 0.5059232711791992, 0.506276547908783, 0.5071367621421814, 0.507724940776825, 0.5084269642829895, 0.508355975151062, 0.5091527700424194, 0.5087148547172546, 0.50924152135849, 0.5100438594818115, 0.5101565718650818, 0.5101882219314575, 0.5101410150527954, 0.5101673007011414, 0.5101878046989441, 0.5102726817131042, 0.5104220509529114, 0.5103794932365417, 0.5103233456611633], 'val_outputs_loss': [0.6606046557426453, 0.6577965617179871, 0.6536424160003662, 0.6359769701957703, 0.5600216388702393, 0.4392385482788086, 0.32422080636024475, 0.31932350993156433, 0.2792191803455353, 0.273492693901062, 0.2692040205001831, 0.25587165355682373, 0.253233939409256, 0.248159721493721, 0.2901904582977295, 0.28424689173698425, 0.24696071445941925, 0.25940945744514465, 0.23389482498168945, 0.25526878237724304, 0.23174995183944702, 0.23286426067352295, 0.2849642336368561, 0.23107276856899261, 0.24503803253173828, 0.29727619886398315, 0.22665099799633026, 0.22901476919651031, 0.22290287911891937, 0.23289456963539124, 0.22006438672542572, 0.2392532378435135, 0.22879721224308014, 0.2347145825624466, 0.2202894389629364, 0.2180243730545044, 0.22763709723949432, 0.2190815955400467, 0.2419801503419876, 0.23307494819164276, 0.21427412331104279, 0.21974334120750427, 0.24457451701164246, 0.22543112933635712, 0.26021793484687805, 0.2156161665916443, 0.2418389469385147, 0.23537997901439667, 0.24054749310016632, 0.2126963883638382, 0.22220449149608612, 0.22186437249183655, 0.21188238263130188, 0.25332361459732056, 0.22389931976795197, 0.21581478416919708, 0.23160232603549957, 0.2376226931810379, 0.22977924346923828, 0.2288099229335785, 0.244029700756073, 0.2317112237215042, 0.22017934918403625, 0.23315687477588654, 0.22867242991924286, 0.2289130538702011, 0.23772163689136505, 0.24852870404720306, 0.22302813827991486, 0.2276143878698349, 0.23655802011489868, 0.23343120515346527, 0.23843631148338318, 0.255592405796051, 0.24227775633335114, 0.25949826836586, 0.24754011631011963, 0.24288001656532288, 0.24468624591827393, 0.24762335419654846, 0.24945057928562164, 0.24814026057720184, 0.24853485822677612, 0.2494603544473648, 0.2501026391983032, 0.25087031722068787, 0.2508610785007477, 0.25171834230422974, 0.2513442635536194, 0.2519382834434509, 0.2528156638145447, 0.25293925404548645, 0.2529795467853546, 0.252938836812973, 0.2529744505882263, 0.2530028820037842, 0.2530977129936218, 0.25325673818588257, 0.2532237768173218, 0.2531777620315552], 'val_rri_loss': [0.058238301426172256, 0.05594547837972641, 0.055038031190633774, 0.0553506463766098, 0.05872909352183342, 0.06449412554502487, 0.05757218971848488, 0.05758355185389519, 0.05560796335339546, 0.059165265411138535, 0.05492783710360527, 0.055385131388902664, 0.0578923374414444, 0.055402472615242004, 0.05616872385144234, 0.053891707211732864, 0.05425312742590904, 0.05382825434207916, 0.053485676646232605, 0.055275171995162964, 0.05505542829632759, 0.05353207886219025, 0.0539570078253746, 0.054292257875204086, 0.054149217903614044, 0.05459671840071678, 0.05390269681811333, 0.05345877632498741, 0.054075688123703, 0.052989013493061066, 0.05517144873738289, 0.053017426282167435, 0.054029788821935654, 0.05562210828065872, 0.05385924503207207, 0.05316729471087456, 0.05273338407278061, 0.05332920700311661, 0.05301506444811821, 0.05267378315329552, 0.05257453769445419, 0.0526946596801281, 0.052953433245420456, 0.05509718134999275, 0.05258718505501747, 0.052963536232709885, 0.05279255658388138, 0.05247291177511215, 0.05243085324764252, 0.05263639986515045, 0.05275542661547661, 0.052710697054862976, 0.05285896360874176, 0.0529351569712162, 0.05244950205087662, 0.0523981899023056, 0.052958693355321884, 0.05240065976977348, 0.052353519946336746, 0.053145814687013626, 0.05237409099936485, 0.05253120884299278, 0.052473120391368866, 0.052391424775123596, 0.05276654288172722, 0.0523747056722641, 0.05238392949104309, 0.05240636318922043, 0.05265030264854431, 0.05237838253378868, 0.05233367159962654, 0.05224529653787613, 0.05222221091389656, 0.05221517011523247, 0.05221253260970116, 0.05219137668609619, 0.052203524857759476, 0.05217983201146126, 0.052169397473335266, 0.0522228442132473, 0.05221554636955261, 0.05216167867183685, 0.05215306207537651, 0.05215126648545265, 0.05214857682585716, 0.05214482173323631, 0.05213850364089012, 0.05213555693626404, 0.052135929465293884, 0.05213369056582451, 0.052131228148937225, 0.05213083326816559, 0.05213049799203873, 0.05213015899062157, 0.05213012173771858, 0.052130017429590225, 0.05212980508804321, 0.052129391580820084, 0.05212941765785217, 0.052128881216049194], 'val_ampl_loss': [0.07340002059936523, 0.0552542544901371, 0.04258240386843681, 0.03651212900876999, 0.031083054840564728, 0.03482222184538841, 0.02696201577782631, 0.027155065909028053, 0.022000405937433243, 0.02090476267039776, 0.020153354853391647, 0.018741779029369354, 0.017879115417599678, 0.01700293831527233, 0.01847922056913376, 0.017883513122797012, 0.01812613010406494, 0.016806388273835182, 0.017644843086600304, 0.01813765987753868, 0.015019150450825691, 0.015162393450737, 0.018017951399087906, 0.015237043611705303, 0.01448403112590313, 0.016414029523730278, 0.015458877198398113, 0.014219354838132858, 0.01565839909017086, 0.014467200264334679, 0.013859354890882969, 0.013989198952913284, 0.016390182077884674, 0.01415281742811203, 0.014694801531732082, 0.01387768890708685, 0.015849048271775246, 0.014174372889101505, 0.01431641448289156, 0.013774512335658073, 0.01448206789791584, 0.014067935757339, 0.018959887325763702, 0.01525220088660717, 0.014940217137336731, 0.014095956459641457, 0.014752238057553768, 0.014463073574006557, 0.013871082104742527, 0.014937319792807102, 0.014006261713802814, 0.014452032744884491, 0.014511713758111, 0.013726689852774143, 0.015043648891150951, 0.014891819097101688, 0.014463699422776699, 0.013675406575202942, 0.01375347189605236, 0.014163689687848091, 0.014229666441679, 0.014112960547208786, 0.013889610767364502, 0.014162232168018818, 0.014678393490612507, 0.01554073765873909, 0.013987666927278042, 0.013929622247815132, 0.01529192365705967, 0.013909805566072464, 0.014500149525702, 0.013804532587528229, 0.013848265632987022, 0.013913369737565517, 0.013907887041568756, 0.013948818668723106, 0.013919109478592873, 0.013911137357354164, 0.013914541341364384, 0.013915921561419964, 0.01404713187366724, 0.013912215828895569, 0.01392497681081295, 0.013913482427597046, 0.013914994895458221, 0.013909932225942612, 0.013915049843490124, 0.013918120414018631, 0.013919013552367687, 0.013921583071351051, 0.013920102268457413, 0.013916855677962303, 0.013915951363742352, 0.01391797699034214, 0.013916649855673313, 0.013917338103055954, 0.013916694559156895, 0.01391662284731865, 0.013916653580963612, 0.013917110860347748], 'val_outputs_accuracy': [0.6124850511550903, 0.6124850511550903, 0.6124850511550903, 0.6122856140136719, 0.7297566533088684, 0.7999601364135742, 0.8657758235931396, 0.8599920272827148, 0.8919026851654053, 0.8988831043243408, 0.9002792239189148, 0.9060630202293396, 0.9064618945121765, 0.9074591398239136, 0.8921021223068237, 0.8883126974105835, 0.911647379398346, 0.9072597026824951, 0.9142401218414307, 0.9022736549377441, 0.915237307548523, 0.91623455286026, 0.8925009965896606, 0.9158356785774231, 0.9118468165397644, 0.8817311525344849, 0.9180295467376709, 0.9144395589828491, 0.9182289838790894, 0.9142401218414307, 0.9192261695861816, 0.9144395589828491, 0.9178300499916077, 0.9142401218414307, 0.9206222295761108, 0.9200239181518555, 0.9172317385673523, 0.9194256067276001, 0.9074591398239136, 0.9132429361343384, 0.9190267324447632, 0.9170323014259338, 0.911647379398346, 0.9182289838790894, 0.8986836671829224, 0.9200239181518555, 0.9092540740966797, 0.9134423732757568, 0.9120462536811829, 0.9240127801895142, 0.9174311757087708, 0.9172317385673523, 0.9178300499916077, 0.9088551998138428, 0.9178300499916077, 0.9190267324447632, 0.9160351157188416, 0.9120462536811829, 0.9140406847000122, 0.9154368042945862, 0.9108496308326721, 0.9170323014259338, 0.9210211634635925, 0.9176306128501892, 0.9192261695861816, 0.9188272953033447, 0.9186278581619263, 0.9140406847000122, 0.9192261695861816, 0.9170323014259338, 0.9184284210205078, 0.9186278581619263, 0.9186278581619263, 0.9154368042945862, 0.9154368042945862, 0.9100518822669983, 0.914838433265686, 0.9172317385673523, 0.9170323014259338, 0.9158356785774231, 0.915237307548523, 0.9146389961242676, 0.9150378704071045, 0.915237307548523, 0.9156362414360046, 0.914838433265686, 0.914838433265686, 0.9160351157188416, 0.9158356785774231, 0.9158356785774231, 0.9156362414360046, 0.9150378704071045, 0.9150378704071045, 0.914838433265686, 0.914838433265686, 0.9154368042945862, 0.9150378704071045, 0.9150378704071045, 0.915237307548523, 0.9150378704071045], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.000100000005, 0.000100000005, 0.000100000005, 0.000100000005, 0.000100000005, 0.000100000005, 0.000100000005, 0.000100000005, 0.000100000005, 0.000100000005, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-06, 1.0000001e-06, 1.0000001e-06, 1.0000001e-06, 1.0000001e-06, 1.0000001e-06, 1.0000001e-06, 1.0000001e-06, 1.0000001e-06]}\n",
      "530/530 [==============================] - 3s 6ms/step - loss: 0.5380 - outputs_loss: 0.2804 - rri_loss: 0.0527 - ampl_loss: 0.0138 - outputs_accuracy: 0.9052\n",
      "17/17 [==============================] - 1s 46ms/step\n",
      "17/17 [==============================] - 1s 45ms/step\n",
      "TP:5604, TN:9735, FP:720, FN:887, loss0.5379825234413147, acc0.9051693615012393, sn0.8633492528115853, sp0.9311334289813487, f10.8746000780335543, roc0.9590743973132501\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAGGCAYAAABmGOKbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACKtElEQVR4nO3deVwU9f8H8Ndyg8glCIooed9iHoia1Tc8y9TM1CyUzMqjTDoU7yyl0+wwKZO00jTNytI0w7SfeZBX5n2LFwgot5w7vz/esrCwwAJ7sPB6Ph5T7uzM7HtGcPY9n8/n/VEpiqKAiIiIiIiIiAzOytwBEBEREREREdVUTLqJiIiIiIiIjIRJNxEREREREZGRMOkmIiIiIiIiMhIm3URERERERERGwqSbiIiIiIiIyEiYdBMREREREREZCZNuIiIiIiIiIiNh0k1ERERERERkJEy6iYiIiIiIiIyESTdRDbRy5UqoVCocOHDA3KEQERHVCp999hlUKhUCAwPNHQoRVTNMuomIiIiIqmj16tXw9/dHTEwMzp07Z+5wiKgaYdJNRERERFQFFy9exJ49e7B48WJ4eXlh9erV5g5Jp4yMDHOHQFQrMekmqqUOHz6MgQMHwsXFBc7OznjooYewb98+rW1yc3PxxhtvoEWLFnBwcEC9evXQu3dvbN++XbNNXFwcQkND0ahRI9jb26NBgwYYMmQILl26ZOIzIiIiMo/Vq1fD3d0dDz/8MB5//HGdSXdycjKmTZsGf39/2Nvbo1GjRggJCUFiYqJmm6ysLMyfPx8tW7aEg4MDGjRogMceewznz58HAOzcuRMqlQo7d+7UOvalS5egUqmwcuVKzbpx48bB2dkZ58+fx6BBg1C3bl2MGTMGAPB///d/GDFiBBo3bgx7e3v4+flh2rRpuHPnTom4T506hSeeeAJeXl5wdHREq1atMGvWLADAn3/+CZVKhR9//LHEfmvWrIFKpcLevXsrfD2JahobcwdARKZ3/Phx3HfffXBxccHrr78OW1tbfP7553jggQewa9cuzXi0+fPnIyIiAs8++yy6d++O1NRUHDhwAIcOHULfvn0BAMOHD8fx48fx4osvwt/fHzdv3sT27dsRGxsLf39/M54lERGRaaxevRqPPfYY7OzsMHr0aCxbtgz//PMPunXrBgBIT0/Hfffdh5MnT+KZZ57Bvffei8TERGzatAlXr16Fp6cn8vPz8cgjjyA6OhqjRo3C1KlTkZaWhu3bt+PYsWNo1qxZhePKy8tD//790bt3b7z//vtwcnICAKxfvx6ZmZmYOHEi6tWrh5iYGHzyySe4evUq1q9fr9n/6NGjuO+++2Bra4vnnnsO/v7+OH/+PH755RcsXLgQDzzwAPz8/LB69WoMGzasxDVp1qwZgoKCqnBliWoIhYhqnK+++koBoPzzzz863x86dKhiZ2ennD9/XrPu+vXrSt26dZU+ffpo1nXq1El5+OGHS/2c27dvKwCU9957z3DBExERWZADBw4oAJTt27criqIoarVaadSokTJ16lTNNnPnzlUAKBs3biyxv1qtVhRFUaKiohQAyuLFi0vd5s8//1QAKH/++afW+xcvXlQAKF999ZVm3dixYxUAyowZM0ocLzMzs8S6iIgIRaVSKZcvX9as69Onj1K3bl2tdUXjURRFCQ8PV+zt7ZXk5GTNups3byo2NjbKvHnzSnwOUW3E7uVEtUx+fj5+//13DB06FE2bNtWsb9CgAZ588kns3r0bqampAAA3NzccP34cZ8+e1XksR0dH2NnZYefOnbh9+7ZJ4iciIqpOVq9eDW9vbzz44IMAAJVKhZEjR2Lt2rXIz88HAPzwww/o1KlTidbggu0LtvH09MSLL75Y6jaVMXHixBLrHB0dNX/OyMhAYmIievbsCUVRcPjwYQBAQkIC/vrrLzzzzDNo3LhxqfGEhIQgOzsbGzZs0Kxbt24d8vLy8NRTT1U6bqKahEk3US2TkJCAzMxMtGrVqsR7bdq0gVqtxpUrVwAACxYsQHJyMlq2bIkOHTrgtddew9GjRzXb29vb45133sFvv/0Gb29v9OnTB++++y7i4uJMdj5ERETmkp+fj7Vr1+LBBx/ExYsXce7cOZw7dw6BgYGIj49HdHQ0AOD8+fNo3759mcc6f/48WrVqBRsbw43+tLGxQaNGjUqsj42Nxbhx4+Dh4QFnZ2d4eXnh/vvvBwCkpKQAAC5cuAAA5cbdunVrdOvWTWsc++rVq9GjRw80b97cUKdCZNGYdBNRqfr06YPz588jKioK7du3x5dffol7770XX375pWabl19+GWfOnEFERAQcHBwwZ84ctGnTRvOknIiIqKbasWMHbty4gbVr16JFixaa5YknngAAg1cxL63Fu6BFvTh7e3tYWVmV2LZv377YvHkzpk+fjp9++gnbt2/XFGFTq9UVjiskJAS7du3C1atXcf78eezbt4+t3ERFsJAaUS3j5eUFJycnnD59usR7p06dgpWVFfz8/DTrPDw8EBoaitDQUKSnp6NPnz6YP38+nn32Wc02zZo1wyuvvIJXXnkFZ8+eRUBAAD744AN8++23JjknIiIic1i9ejXq16+PpUuXlnhv48aN+PHHHxEZGYlmzZrh2LFjZR6rWbNm2L9/P3Jzc2Fra6tzG3d3dwBSCb2oy5cv6x3zf//9hzNnzmDVqlUICQnRrC86MwkAzRC08uIGgFGjRiEsLAzfffcd7ty5A1tbW4wcOVLvmIhqOrZ0E9Uy1tbW6NevH37++Wetab3i4+OxZs0a9O7dGy4uLgCApKQkrX2dnZ3RvHlzZGdnAwAyMzORlZWltU2zZs1Qt25dzTZEREQ10Z07d7Bx40Y88sgjePzxx0ssU6ZMQVpaGjZt2oThw4fj33//1Tm1lqIoAGQ2kMTERHz66aelbtOkSRNYW1vjr7/+0nr/s88+0ztua2trrWMW/Pmjjz7S2s7Lywt9+vRBVFQUYmNjdcZTwNPTEwMHDsS3336L1atXY8CAAfD09NQ7JqKaji3dRDVYVFQUtm7dWmL9/PnzsX37dvTu3RuTJk2CjY0NPv/8c2RnZ+Pdd9/VbNe2bVs88MAD6NKlCzw8PHDgwAFs2LABU6ZMAQCcOXMGDz30EJ544gm0bdsWNjY2+PHHHxEfH49Ro0aZ7DyJiIhMbdOmTUhLS8Ojjz6q8/0ePXrAy8sLq1evxpo1a7BhwwaMGDECzzzzDLp06YJbt25h06ZNiIyMRKdOnRASEoKvv/4aYWFhiImJwX333YeMjAz88ccfmDRpEoYMGQJXV1eMGDECn3zyCVQqFZo1a4Zff/0VN2/e1Dvu1q1bo1mzZnj11Vdx7do1uLi44IcfftBZEPXjjz9G7969ce+99+K5557DPffcg0uXLmHz5s04cuSI1rYhISF4/PHHAQBvvvmm/heSqDYwZ+l0IjKOginDSluuXLmiHDp0SOnfv7/i7OysODk5KQ8++KCyZ88ereO89dZbSvfu3RU3NzfF0dFRad26tbJw4UIlJydHURRFSUxMVCZPnqy0bt1aqVOnjuLq6qoEBgYq33//vTlOm4iIyGQGDx6sODg4KBkZGaVuM27cOMXW1lZJTExUkpKSlClTpii+vr6KnZ2d0qhRI2Xs2LFKYmKiZvvMzExl1qxZyj333KPY2toqPj4+yuOPP641xWdCQoIyfPhwxcnJSXF3d1eef/555dixYzqnDKtTp47OuE6cOKEEBwcrzs7OiqenpzJhwgTl33//LXEMRVGUY8eOKcOGDVPc3NwUBwcHpVWrVsqcOXNKHDM7O1txd3dXXF1dlTt37uh5FYlqB5WiFOsfQkREREREVAF5eXlo2LAhBg8ejBUrVpg7HKJqhWO6iYiIiIioSn766SckJCRoFWcjIsGWbiIiIiIiqpT9+/fj6NGjePPNN+Hp6YlDhw6ZOySiaoct3UREREREVCnLli3DxIkTUb9+fXz99dfmDoeoWmJLNxEREREREZGRsKWbiIiIiIiIyEiYdBMREREREREZiU1ldlq6dCnee+89xMXFoVOnTvjkk0/QvXt3nds+8MAD2LVrV4n1gwYNwubNm/X6PLVajevXr6Nu3bpQqVSVCZmIiKhaUhQFaWlpaNiwIaysat6zcN7DiYioptL3Hl7hpHvdunUICwtDZGQkAgMDsWTJEvTv3x+nT59G/fr1S2y/ceNG5OTkaF4nJSWhU6dOGDFihN6fef36dfj5+VU0VCIiIotx5coVNGrUyNxhGBzv4UREVNOVdw+vcCG1wMBAdOvWDZ9++ikAeYLt5+eHF198ETNmzCh3/yVLlmDu3Lm4ceMG6tSpo9dnpqSkwM3NDVeuXIGLi0tFwiUiIqrWUlNT4efnh+TkZLi6upo7HIPjPZyIiGoqfe/hFWrpzsnJwcGDBxEeHq5ZZ2VlheDgYOzdu1evY6xYsQKjRo3SO+EGoOmO5uLiwhs2ERHVSDW16zXv4UREVNOVdw+vUNKdmJiI/Px8eHt7a6339vbGqVOnyt0/JiYGx44dw4oVK8rcLjs7G9nZ2ZrXqampFQmTiIiIiIiIqFowacWWFStWoEOHDqUWXSsQEREBV1dXzcKxYERERERERGSJKpR0e3p6wtraGvHx8Vrr4+Pj4ePjU+a+GRkZWLt2LcaPH1/u54SHhyMlJUWzXLlypSJhEhEREREREVULFUq67ezs0KVLF0RHR2vWqdVqREdHIygoqMx9169fj+zsbDz11FPlfo69vb1m7BfHgBEREREREZGlqvCUYWFhYRg7diy6du2K7t27Y8mSJcjIyEBoaCgAICQkBL6+voiIiNDab8WKFRg6dCjq1atnmMiJiIiIiIiIqrkKJ90jR45EQkIC5s6di7i4OAQEBGDr1q2a4mqxsbElJgY/ffo0du/ejd9//90wURMRERERERFZgArP020OqampcHV1RUpKCruaExFRjVLT73E1/fyIiKj20vceZ9Lq5URERERERES1CZNuIiIiIiIiIiNh0k1ERERERERkJEy6iYiIiIiIiIykwtXLa4R//wXatgVsbc0dCRERERERUc2WkgKcPQskJwOenoCXl/zf3l739nl5QHo6kJUFWFtL3mZjA1hZAfn5QG6ubJOfL+sLloLtrK0Blcqkp1iW2pl0//03cOIEMHq0uSMhIiIiIiIynOxs4OpVWW7cAOLiCv9/544kqnl5gKJIYurgULg0aAC0aAE0by5Laipw+DBw5Ahw7JjsV1TBMUp7rVbL/11d5bhubkBSEpCYCCQkSKy69rexAZydJSlXq+VzdSXZBUl4wfu5uYWvC45XVL16wFdfGepK6612ThmWmQkMGwZs3VqtnoAQEVHtU9On1Krp50dEZHaJicC6dcBvv0luY28P+PoCjRoBDRsCPj6Fi5NTYUuwlZUktNnZ0qJ85w5w7Rpw7pwsZ88CdesCnTvL0q6dJOakoe89rna2dDs5AT16ADt2AA89ZO5oiIiIiIiIxIULwOefSwuznR3g7w906AC0bi1JdVaWLLduAZs3y7qRI4ENGyqeFFtZAY6Osri7S5LerZtRTqs2q51JNwBMmQI89xyTbiIiIiIiqrr8fCAjQ8YiFx2PXNAV2tkZqF9fd0/blBTgzz+Br7+WrtgvvAC8/bZ0j750Sbp2x8Rodwd3dgaWLZOx0VSt1d6k28tLxiwcPQp07GjuaIiIiIiIyJLcvg389Zcky6dPS6t03bqSDOsaj5ySAsTHSwLepg3g5ydjpa9dA1xcgF69gOXLZdxxAZUKaNpUFrJYtTfpBoBp04CFC4GVK80dCRERERERWQJFAd57D9i9Gxg4UFqlW7XSv1ZUbi5w6hRw+TIwZIiMvaYarXYn3S1aSMGAK1fkSRMREREREVFp8vOl4a5hQ+DnnytXlNnWVsZod+hg+PioWrIydwBm9/LLwEcfmTsKIiIiIiKqzu7cAcaMkUJjM2ZwFiTSW61MutesAb77TuocIChI5uzetUu6ehARERERERV1+zYwfDgwfjzw9NPmjoYsTK1MugcOlKm6R48GQkKALSO+Qv5ffwNDh0q5/S+/lA2IiIiIiKh2S0gAnngCeOstoG9fc0dDFqhWJt3u7vKQatMmICICOJ7ojf67ZmLj+M1QlkXKOIsnn5Qy/0REREREVDvduAGMGgV8+CFw773mjoYsVK1Muovy9QVee03qIJw9CwwY7Y7fG4yF8vwL0nWEXc6JiMhCLF26FP7+/nBwcEBgYCBiYmJK3TY3NxcLFixAs2bN4ODggE6dOmHr1q1a28yfPx8qlUprad26tbFPg4ioerhyRcZwf/YZ0L69uaMhC1brk+4CdeoA06cD69YB69cDP2QMkF+y0FCpUkhERFSNrVu3DmFhYZg3bx4OHTqETp06oX///rh586bO7WfPno3PP/8cn3zyCU6cOIEXXngBw4YNw+HDh7W2a9euHW7cuKFZdu/ebYrTISIyr4sXgbFjZdhpq1bmjoYsHJPuYtzcgMWLgRUrAGXIUODhh4Hnn5eJ7YmIiKqpxYsXY8KECQgNDUXbtm0RGRkJJycnREVF6dz+m2++wcyZMzFo0CA0bdoUEydOxKBBg/DBBx9obWdjYwMfHx/N4unpaYrTISIynzNnZCzqypVA06bmjoZqACbdOtStC3TsCOzZA6m21qQJsHmzucMiIiLSKScnBwcPHkRwcLBmnZWVFYKDg7F3716d+2RnZ8PBwUFrnaOjY4mW7LNnz6Jhw4Zo2rQpxowZg9jY2DJjyc7ORmpqqtZCRGQxTpwAJk4Evv0WaNzY3NFQDcGkuxQvvQR8/PHdF/37AwcPmjUeIiKi0iQmJiI/Px/e3t5a6729vREXF6dzn/79+2Px4sU4e/Ys1Go1tm/fjo0bN+LGjRuabQIDA7Fy5Ups3boVy5Ytw8WLF3HfffchLS2t1FgiIiLg6uqqWfz8/AxzkkRExnbkiCQB330HNGxo7mioBrExdwDVla8v4OgoxdVadOgAvPOOuUMiIiIymI8++ggTJkxA69atoVKp0KxZM4SGhmp1Rx84cKDmzx07dkRgYCCaNGmC77//HuPHj9d53PDwcISFhWlep6amMvEmouonORmIiQESE2VJSAD27we+/x7w8DB3dFTDsKW7DNOmAR99BMm+OX0YERFVU56enrC2tkZ8fLzW+vj4ePj4+Ojcx8vLCz/99BMyMjJw+fJlnDp1Cs7OzmhaxvhFNzc3tGzZEufOnSt1G3t7e7i4uGgtREQGl5lZerFjRQG2bQP27dP9/i+/AI89Bpw8CahUQJs2wPDhMp0RE24yAibdZejUSWYKSEqC/AImJZk7JCIiohLs7OzQpUsXREdHa9ap1WpER0cjKCiozH0dHBzg6+uLvLw8/PDDDxgyZEip26anp+P8+fNo0KCBwWInIqqwM2eAgQOBfv2ATz4B0tML39u5Exg0SBLuDRuARx8Ftm+XRPz2bZmZ6P/+T+o1TZ0q9Zv69gUCAqShjcgI2L28HC+8AERGArM6dwYOHwaKFKkhIiKqLsLCwjB27Fh07doV3bt3x5IlS5CRkYHQ0FAAQEhICHx9fREREQEA2L9/P65du4aAgABcu3YN8+fPh1qtxuuvv6455quvvorBgwejSZMmuH79OubNmwdra2uMHj3aLOdIRISDB2We37VrAS8vYONG4PHHgXbtgHPngNatpQhavXqyfVKSJObvvw/k5QELFwI9epj3HKjWYdJdjgEDgA8+AF55rQscDscw6SYiompp5MiRSEhIwNy5cxEXF4eAgABs3bpVU1wtNjYWVlaFHdyysrIwe/ZsXLhwAc7Ozhg0aBC++eYbuLm5aba5evUqRo8ejaSkJHh5eaF3797Yt28fvLy8TH16RFST3LoFvPwy0KwZMGIE0Latfvv9+Sfw3nvA+vWAu7use+IJWf75R4oyFS+AVq8eMH++dEe3tgbs7Q15JkR6USmKopg7iPKkpqbC1dUVKSkpZhkbtngx0K5xGvr/+AKwerXJP5+IiGouc9/jjK2mnx8RVdCFC8CECcC77wIZGZJAnzoF9OoFDB4MdO4MWBUbAXvzpnQV37ED+PprwMnJPLETFaPvPY4t3Xpo2hS4klhXqhwSEREREdUkBeOd9SkilpEhCfPvvwPLlwPt2+v/OTExwMyZwFdfFc6B3acPoFYDe/YAP/wAzJoF+PgA//ufdBf/5x9pre7fX6bysrWt3DkSmRGTbj34+gJHj0KeqmVkAHXqmDskIiIiIqKqUxSZm/rAAWD8eFlUqpLbqdXS4/Orr4CwMGDKFGDMGOm63bNn2Z+hVkuL9po1kli7umq/b2UF9O4tCwBcvy5dyQcNAubNk27hRBaM1cv14OsLXLsGoGNH4L//zB0OEREREVHVKQrw4otShGz3bpmr+qmntHt33r4NrFghhY6SkoCtW4FHHpEiZj/8ALz9NvDbb7qPfegQ8OqrUmX87FlJvIsn3Lo0bCgJfY8eTLipRmBLtx68vYH4eACD71YwZ8VDIiIiIrJkarUk3B07As8/L+vCw2Wqrccek4rgf/4pyfNjj0mVcGdn7WPUrSuJ9LhxwOnTMuXW2bMybjszE+jQAXjySSl+pqv1nKiWYNKtB2trID8fUthh0yZzh0NEREREVHlZWcArr2gn3AV69AB+/BHYskXmzS2Yeqs09vYyRdfq1VJR/P77gXvuYZVwoiKYdFdEw4YyxoSIiIiIqDpRlNJbkzMzpav3n39KMTMrK2mBHjlS9/aursDo0fp/trU1EBJS8ZiJagkm3XqyswNyclWws7EBcnNZOZGIiIiIzO/OHWDiRBkLaW0thX+bNwdsbICTJyXhdnICOnWSiuDTp8sXWyIyGSbdemrQALhxA2jSurXMJdihg7lDIiIiIqKaKCkJuHwZcHCQxdFRCpfZFPvqfu0a8Mwz0lW8Xz9Zl5EBnD8vjUQzZnBOa6JqgEm3ngoqmDfpfLeYGpNuIiIiotqnYE5pHx+gWbPyC4Tl5gI//SRDFG1sZLG1lemxWrYsue3SpcC2bUBgoIy9zsqS1uxLl2T70aOBoCCZvzo8HFi2TPs4derIWG0iqjaYdOupYcO704Z17iz/uHHcChEREVHtkZkJrFol02T16iXTap0/L12627UDunYFunUDGjWSRPzWLeCLL4A//pDq3+3bA3l5smRlAR98AFy5AgwfDjzxBLB/PxARAYwdC2zeLOOuizt+HPjuO2DuXBl3/cMPgJubqa8EEVUQk249+frKv3MY3hw4d87c4RARERFRZSQkAB9+CDz3HODvX/72ubmSDP/9tyTEv/2mXdsnN1e+JB44ALz1FnD1qhQ1s7WVz3j9dd0J9PDhkshv3ChzY7dsKVXDXVxKj6VdO/mMsoqmEVG1w6RbT76+wO+/Q/7RVBT+Y0dERERkaWJjZU7pl14Cpk0D2raVcc916+rePj4eGD8eePppYM4c3d/9bG2BgABZnn1W1uXnSwt4eZycJOF+6qmKnQe/gxJZFB2P3UiXgjHdAGTuwYsXzRoPERERUa2Tmwu88w4waBCwcqW0FOvrxAlJuKOigKFDpVW5d29gyBCZjzouTnv7/ftl/PTbb8vUWhVJdPVJuA3o1Clg+3aTfiQRVUClku6lS5fC398fDg4OCAwMRExMTJnbJycnY/LkyWjQoAHs7e3RsmVLbNmypVIBm0vdukBq6t0XHToA//1n1niIiIiILNL588Dq1dIlOy9P//3+7/+AgQMBPz9g/XpZN2IEMHmydEfcvl26fv/yi4yJPnBAWkxycyWBfvllYO1a7S7lAwdK0TJnZ2nxHjAAmDQJeOMN4L33pOt3+/aGPHujWLgQWLQIOHrU3JEQkS4V7l6+bt06hIWFITIyEoGBgViyZAn69++P06dPo379+iW2z8nJQd++fVG/fn1s2LABvr6+uHz5MtwsrOiD1sNNb28ZD0RERERE+ouNBSZMkBbnVaukiTYvD3joIWDq1JJTYgFAYqJU6bazk2Tb3V3Wjxsny/HjUqzMykpTHTwvRw2bAwek9To+XlpPNmzQPV7a1rawi7eiSG/GkyelO7musdjVzOXLUlD9+++lQX7dOpldzNIdOiTD3J2dzR0JUdVVOOlevHgxJkyYgNDQUABAZGQkNm/ejKioKMyYMaPE9lFRUbh16xb27NkD27tFJ/z1KVpRDalUd4dye3gAp0+bOxwiIiIiy3HzpiTJK1bIUL0CigJ8/TXwyCPA4sUyzhqQZHzZMuDXX4EFC2QKLV3atZPlrp9+At59F6hXT15bWQGtvIGX04GGpdQou337bi6vUgFNm8piQjt3SgN8Zb4if/SRPK/w8gKWLAFCQ6WB3s7OsDGaSk4OMGuWPCu5elWmIH/44ZLbsbwSWZIKJd05OTk4ePAgwsPDNeusrKwQHByMvXv36txn06ZNCAoKwuTJk/Hzzz/Dy8sLTz75JKZPnw5rE493qSpXVyAlBXCrVw9ISjJ3OERERETVTm6uJLpaX/NSU6Ul+ZNPkOx+D9S3AA+Pu++pVFIVPDgY6hen4vXE13A6tylW2U6Ax7hHpcu4ni3Ohw5JTr9zZ2HSmZcHHDwITJkiufT06ZKgZmRIq/D330sv9L//LrtweIGUFEls/f0l7E6dyt/n+nWZ1ru001i6VKbXXrlSr9PUSE6WdqDu3eV1x45S9+3ll4HPPqvYsYq7c0eu5969QEyMXK+XXgL696/accty8SIwcaLUo3v8cRmyv2ABsGYN8P77gKOjjCTYvFmS8uIPFqytZRsnJ1lCQmQmt+Ly8qSwfGysDOl/+mntmdfu3JGp2E+elBju3JH/5+ZqH6dFC4m1aDF7Il0qlHQnJiYiPz8f3t7eWuu9vb1x6tQpnftcuHABO3bswJgxY7BlyxacO3cOkyZNQm5uLubNm6dzn+zsbGRnZ2tep2oGU5tXQTE1t3oeMvciERERUU2TkgKkp0u/3jp1JJOJjZXsa+9e4MwZaUYePlzGQDs6au3+0UfAzz/LFNVt2kAyliefBN58E9uvt0PEi5Jnh4cDwcGF++V5++I51/Xo4boPY/AdRlxYj6iHbNGkSKKalia9vo8fl/pmXboUvnftGvDqq9KLvGgyZmMjjeQbN8qw8NBQOa20NGDUKKmn9ttvwJdfAmFhZV+azEw5lXnz5DNWrpRYBg2SpF5X7/grV4CgIEkc+/Qp+b5aLQltnTqSQLdqVXYMRS1fLsljUcOGSUyPPgrUr1+YhOblSctxQe25evVKT/L/+w94/nk5r6Ag+bOiSA27ZcskEe7YUf849fHzz3LsZcsKO0I4Ocnf89GjEoO9vfzIvf020KBByWPk5xcmyCkp8vMwdqxMk14gJ0fWDR8uCfemTfLaywto0kQeMFhby1TsHTrI30vBNbS11W5d371bygLMnAn8739VO/8LF+QhEFD4eS4uQOfO0s2erfqWTaUoiqLvxtevX4evry/27NmDoKAgzfrXX38du3btwv79+0vs07JlS2RlZeHixYualu3Fixfjvffew40bN3R+zvz58/HGG2+UWJ+SkgIXfR5BGsnHHwOtWwP9HsiRf6U3bjRbLEREVDOkpqbC1dXV7Pc4Y6np52eJcnOlBe/ff4EjR4CzZ6W+mJMTpPlw5Ejp4p2eDqSnY//Ne+DX1BYNH2ojGViLFjJW+ocfgK1bpYlwxgxNwbFnn5VaZIsWAd3aZ+KVmFHIe3Eawn9/EDk50vUbkNZYb29g/nyJaexY4IknpIUTAC5dAp55RnqcBwRIUvbRR9L1uFMnmfGrSRNJwvPyJNlculTCK4uiSPhFk7b8fKBfPzmd0lotc3KkmPnkydoJVm4u8M03QHS0DFMvmnhnZkpiN2GCXO+FC0se98gRuf6hoTIduL6t3Tk5kvBt3667BT09XT6/oKXW2loakOrUkfdnzpTEsnjXbUWRnv5ffCHbF3flSuFDh/ff12/MtaLI0PzSxpr/8Yf0UPj6a8O2GufmAi++KEn8668DWVnS4eKZZ0qe99mzMgKiW7eKdc1PSZGHEDduSFd4J6fC97y8AE/P0vdVFOlh8emn8pnjxgEODoUPDpKTpR7g2bPywKFzZ+mt0aiR1BP09ZXt9aVWyzGTkmS5dUsevrRsWVgqQZeEBGDfPlmSkuQcHR1lKf4gwsam8KGBo6Mct1EjWUqbmU8fiiLx3rol/+S4u+t+yKVrP2M+sND3HlehpDsnJwdOTk7YsGEDhg4dqlk/duxYJCcn4+effy6xz/333w9bW1v88ccfmnW//fYbBg0ahOzsbNjp+KnW1dLt5+dn9hv2Dz/IU9Fx4wAMHizVMYmIiKqgpielNf38qqPYWKBxY93vKYp0D+7SRRLXgAD5fnPffUCf7lmSuX7wQeG4akjSmJEhrYYDBug46JUrktkMGAA8/zwGP6rCpk0ALl7EN4+swxq3ichzcsW0aSUTnTVrgG+/laRx6lSgb1/t95OSJEmys5NWx1mztBvWf/hBWkadnKSV+oEHKnHB7lq6VLq8jx5d8r38fPn+9/jjcj102bBBWs1XrpRERFGk23JoKPDgg3Luv/1Wcr8PP5QeAQMGyGeEh+vX2v3tt5JYv/BCBU6yiIwM+Tq7ZYt24vbtt/JQ4tVXy95/1y4p8l7wUKS42FhJpv/8UxJuKyuZoW3GDO0k6OhR6fK/cWOJThMGoSgS46lT8kzppZe0e1gYysmT8tBArS5c988/pV+fixflAU6nTvL/Ro3KPv6dO3KtLl2SHgsFS3a2nKONjTzEunNHEmtdGZ5KJQlrvXqyuLvL79iZM1LXANB+gKMocj716gE9esgzt/r1Cx8KZGaWnIAgN1feL1hu3ZI4r1yRn9eCOIp+RkHdrKJxFn2/gIeHLMnJEq9aXTKpLv66+LHr1QO++qr061xRet/jlArq3r27MmXKFM3r/Px8xdfXV4mIiNC5fXh4uNKkSRMlPz9fs27JkiVKgwYN9P7MlJQUBYCSkpJS0XANau9eRXnrrbsvBg82ayxERFQzVJd7nLHU9POrbv78KVlxs05RLr39naKo1SXe371bUWbM0F63a5eiRCxSK0poqKJs2aL1Xl6eogwapCiZmYoycaKivP66ouTk6Pjg/HxFefddRRk1Snmkf7aixMQoyv/+pyiXLyvx8YqSmFh6zOfOKcp//5X+fmamoly6VPr7CQmK8vffpb+vr/R0RenXr+RlU6vl3FetKv8YGzYoypNPyjVauFBRPv648L0xYxTl5s2S+zz2mKKkpcmfT51SlJCQkp//1VeK8tprivLbb7KtWq0offsqSkZGhU6xhB9+UJQ33ih8nZwsf206/451uHlT4v/0U4np9m1F+eIL+ZkJDVWUdesUJT6+8Dw+/FBRnnlGUbKzZd3Vq4ry4IPyd2hsv/2mKHv2GP9zikpKUpTgYEU5dEh7/cGDcp3L+rmuqJwcuZ5JSfLrSKah7z2uwvMghIWFYfny5Vi1ahVOnjyJiRMnIiMjQ1PNPCQkRKvQ2sSJE3Hr1i1MnToVZ86cwebNm7Fo0SJMnjy5oh9tdgVjuomIiIjM5cQJ3euvX1Pw5nNX8MPH1/DRlhYykDU2tnCDjAxETr+IF45Okn7B168DkO60/6w5I93DBw7UOuaxY9LC7Ogohbm6dpXW0ZSUYh9uZQW89hpSJrwKl8O7gDfflGboxo1Rv35hJXFdmjUreypsR0fpRl4aT0+gZ8/S39dXnTpyLf76S3v9zJkyvDAkpPxjDB8ul71vX5nKa8qUwvf69pWW36Ly86XLc0EX7VatpGWuoFTSnTvSNf3qVWDMGFk/dqx0b+/dW7src2UMGwYcPiytroB0G589W/8u3l5eMotbVpb0lggNlXNZvx6IipLhAgUzCqtUMqRg6FD53EuXpGX/iy/K7oJtKAMGSEutKXl4yDjt6dPlOgMyLfycOXKNyvq5rihbW8lVPDwsYqa72qcyGf0nn3yiNG7cWLGzs1O6d++u7Nu3T/Pe/fffr4wdO1Zr+z179iiBgYGKvb290rRpU2XhwoVKXl6e3p9XXZ6S5+QoytChd18MHqzzCTIREVFFVJd7nLHU9PMztbS4dKWBR5YSNk2t5OYWrs/JUZSBHa4oJ1+OVBRFWhqTD5xVlIEDFSUiQlHGjVPi+j6lPN7lgjQ7b94sTa+PPqooc+cqA/2OKur8kt9rli5VlE2btNd9+qmi/Pqr7vhiYhRl/uxcaSK3QDduKMrw4YWv33lHuyVYXzExha25Ba5dU5Rx47TXHTigKDNnaq87fVpau8+fl1bSbdtKHj8313BfQ8+cUZQRIxTlyBFFeeqpyh+n6M9jeQ4fVpROnUzf8mwuBS3e8+cryujRinLnjrkjIkPR9x5X4Xm6AWDKlCmYUvTRXRE7d+4ssS4oKAj79u2rzEdVK7a2RcYtODnJQIaCahRERERExpKbC0RFYf/n5/BK+/vQ8MA1PD5sAqJW2cDDA5gxMRmh1l+j9fvTAUjr6PI/m+PVX36RCmSjRuHL1f6Y0A1APUhZ6kGDpMn6jz/QamhbnDmrKjGW+O+/pZBsUZ06SRVwXXMnnzkDtGxrA1jWrLAaPj4y5vXUKWnxjouTIe4V1a1byXUNG8rxio45/fNPGe9dVMuW0lI5caIUF9M1Pl+fAlL6atFCPvOpp6QVtrIqElNAgBSQqy0KWrxXry6sYUC1iwF/ZWsHzcB8j7vThjHpJiIiokpKTwfyU9LhmnlDMrK4OKkS5OQkpX6dnWXdihXAmDHYPfht9B9ojR53/kSr6VPw+MMfov+jdlD+2IERf47WTI49eLBU45461Rq2jz2GvDxJ8IqMABSursDw4eiplnmJiyfdBdWNi+rQofQ5oM+ckamqLNm0aZKABgRI12dDVj5u376wyz4g13zSpJLbffqpNPZUpIp2VcycKcXFGjY0zefVRh4eUm+Qaicm3RVkbS2t3Tb16km5Pz8/c4dERERE1ZSiyHQ/CQmF0/RcvSrzIGdkAM4JF5B2IRFbQtZKM2uDBvL/zEzZ6eJFGdT866+AoyMODQHCZwGwexD3bmiGtSFjseTbMXhnbkbh5MaQ7yuPPw58/72MBf71V2mZLq2FrVcvYO5cGZNb4OpV3VNGuboCqam6j3PmTPlTdlV37doBr70m47MN3SLZr5+0JnfoIN8ns7N1j8s2dZuOk1PVKr8TUdnYuaGCfHzkgbOmpZuIiKiaWLp0Kfz9/eHg4IDAwEDExMSUum1ubi4WLFiAZs2awcHBAZ06dcLWrVurdEzSpigy7dJ770l37Ph4SVj79pUiU5ufXI11nSLQ4NFuuPTSYplI+OmnpZl65Ehg/HiZR+u55wBHR+TnS+EtTetn48aov/VrLJqZDtvQp0p8/tixMm+0osgUOePGlR5rw4aaumoaf/8txbp0sbOThLG4tDSgJswMN3KkYbtwF7jvPmD3bvnzoUMydRsR1XxMuiuoYcO7FcwLWrqJiIiqgXXr1iEsLAzz5s3DoUOH0KlTJ/Tv3x83b97Uuf3s2bPx+eef45NPPsGJEyfwwgsvYNiwYThcUGK3EsckbQsXSuXm5ctlbuIJE6SydY8egPNv64HffwciIzH6SRXWri3/eP/9V9gtWcPBQZqydfSBdnKSz/r8c4nD3b3s47u7a7cn/P23tIDr0qaNzEtclK55gUmbg4O0nmdm6h7PTUQ1E5PuCvL1vfskmC3dRERUjSxevBgTJkxAaGgo2rZti8jISDg5OSEqKkrn9t988w1mzpyJQYMGoWnTppg4cSIGDRqED4pUjaroManQp5/KeO3poTeBJUukf/f589JU/fPPwI8/yjhta2s8+CCwY0f5x9y9u/SW59JMniyt7brGDRcXFAQUrXt79izQvLnubTt2BI4e1V4XFye946lsffpIkba9e00/hRURmQeT7grSzNXNlm4iIqomcnJycPDgQQQHB2vWWVlZITg4GHv37tW5T3Z2NhwcHLTWOTo6Yvfdvq+VOSaJb76RYlkRb+UDzz4r3eSuXAE++kgmKP7lF+n3fbf/srU10LattGSXpTJJmrc3EB0NdO5c/rY9e0rrNiDdxJ2dSy8ipivpPnOmZCE2KqlfP2DzZhnTXexXkIhqKBZSqyBfX2DnTgB9PZh0ExFRtZCYmIj8/Hx4e3trrff29sapU6d07tO/f38sXrwYffr0QbNmzRAdHY2NGzciPz+/0scEJJnPLjLYN7W0ils1zOXL0lq9Y4dUnV6+HFC98YZUM3viiXL3f/JJ4LvvdHQfL+L2beloV1GBgfpt17EjMG+e/Hn//rL3a9FCkuyizpyRqaeobG3aANu3A6NHmzsSIjIVtnRXkNaYbnYvJyIiC/XRRx+hRYsWaN26Nezs7DBlyhSEhobCqorlmiMiIuDq6qpZ/Gr4LB/bt8tUS4sWyRjqDz6QImnWf2yTymkhIXodp1s34J9/Sh8XfeUK0KiRAQPXwcZGWrZzc8suolaw7d3nMxpMuvWjUgH33y8/N0RUOzDpriB3d3nSXPgHIiIi8/L09IS1tTXi4+O11sfHx8PHx0fnPl5eXvjpp5+QkZGBy5cv49SpU3B2dkbTpk0rfUwACA8PR0pKima5cuVKFc+u+rp8WSqT//qrFCsbOVIKluHKFeD992Ust55UKuk6XlrP/fKSYEPp1Em6jR86BNx7b9nb1q8PFK2pd/48cPfHh8qxdGnpReqIqOZh0l1BKtXd8U3W1iUf8RIREZmBnZ0dunTpgujoaM06tVqN6OhoBJUzCNjBwQG+vr7Iy8vDDz/8gCFDhlTpmPb29nBxcdFaaqLsbJnJKzKy2Ljc27eB558HPvtM5teugNGjgTVrdL+3e7dpkrSePaXIl9bUZKXo2FF7HHpubvn7kDDGdGREVH3xV56IiKgGCAsLw9ixY9G1a1d0794dS5YsQUZGBkJDQwEAISEh8PX1RUREBABg//79uHbtGgICAnDt2jXMnz8farUar7/+ut7HrM2mTQNefBFo6nwTWBApzcN37gBubsArr8ig5wpq0wY4d06SV1tb7fcuXDBNK3JQkFQ7f/TR8rctKKb20ENSFMza2vjxERFZIibdleDsLFU965o7ECIiortGjhyJhIQEzJ07F3FxcQgICMDWrVs1hdBiY2O1xmtnZWVh9uzZuHDhApydnTFo0CB88803cHNz0/uYtdU330hBs0ceVoAnJkuF8ldeAerUqfKxg4Ol2viAAYXryqskbkhubvI5+nRl79gR+PZb+fOlS4C/vxEDIyKyYEy6K6GgmFprlUoqnpjiLkhERFSOKVOmYMqUKTrf27lzp9br+++/HydOnKjSMWujM2eADRuAjRsBrFsnmWf//gY7/qhRksP36gXUvft0f98+oEcPg31EuV58Ub+u7F5eQEKC/JlF1IiISscx3ZXg6wtcvw65G6almTscIiIiMpGtW4GJEwHr24nAl18C06cb9PiNGgEzZwJDhgB79si63btNU0StwPPPS4u3PqyspGs5k24iotIx6a4ETbXOevU4VzcREVEtcvw40L49pDv5228bpXJYnz7Ajz8CX3wBzJkjU4l17mzwjzGIli2Bs2eZdBMRlYVJdyVoZgvz8GDSTUREVItcvQr4Hv4V8PEBunY12ue4ugIrV0qC7+NTsrBadVFQTM0U84gTEVkqJt2VoEm669UDbt0ydzhERERkAooCIC8Xqg8XA/Pnm+QzR44EoqJM8lGVUpB0K4p0NSciopL4z2MlaCXdbOkmIiKqFa5fB3xTTgAvv1zhObhrqjZtgIMHeTmIiMrCpLsStLqXs6WbiIioVjh+HGiX/DcwaJC5Q6k27O2BuLhKTUtORFRrMOmuBDc3IDkZbOkmIiKqRY7/cR3t2iiADWdcLaptWxZRIyIqC5PuSrC3B3JywJZuIiKiWuTY1mtoN6GnucOodh55BAgMNHcURETVFx/VVgVbuomIiGoHtRpx19XwGRBg7kiqnSefNHcERETVG1u6q8LFBUhJMXcUREREZGTKX/8HVT13qKxU5g6FiIgsDJPuqrCyujt/CBEREdVkVz7fAr+u3uYOg4iILBCT7kqysgLy880dBRERERldVhaOnbVH+56u5o6EiIgsEJPuSnJ1Zc9yIiKiWmHLFhz3G4B27cwdCBERWSIm3ZWkmTaMTd5EREQ127p1OG5/L5NuIiKqFCbdleTuDty+Dcm+2eRNRERUM926BeTnIyHNAV5e5g6GiIgsEZPuStIk3Zw2jIiIqOb69VeoHx0KK35jIiKiSuItpJI0SbeHhzwFJyIioppn/35canI//P3NHQgREVkqJt2VxJZuIiKiWuDiRRxPacTx3EREVGlMuitJq6WbSTcREVHNk5MD2Nri2HEV2rc3dzBERGSpmHRXkqZ6eb167F5ORERUQ3z/PfDBB3dfHD8OdOiA48fBlm4iIqo0G3MHYKnY0k1ERFTznDgB7NsH1K0LPId/gK5dkXxE7vtERESVwZbuStIa082WbiIiohohIQGIjARiYoC161TI79yVlcuJiKhK2NJdSY6OwJ07YEs3ERFRDZKQANSvL4n3U41bIv5HXzRrZu6oiIjIkvHZbSWpVHf/4OwMpKebNRYiIiIyjDt3ACcnwCYvC6u6fopfN6vQoYO5oyIiIkvGlu4qUBQUyb6JiIioxjh6FPb3tsNvs80dCBERWTom3URERETFHTgAdO0KG35TIiKiKmL38ipQqe62dhMREZHFU6uLdGD7RyqXExERVVWlku6lS5fC398fDg4OCAwMRExMTKnbrly5EiqVSmtxcHCodMDVSd26QFoaABsbIDfX3OEQEVEtV5H7MwAsWbIErVq1gqOjI/z8/DBt2jRkZWVp3p8/f36Je3jr1q2NfRpmc/t2kanB4uMBHx+zxkNERDVDhTtNrVu3DmFhYYiMjERgYCCWLFmC/v374/Tp06hfv77OfVxcXHD69GnNa1UNGQddMG2Yi4eH/KGU8yciIjK2it6f16xZgxkzZiAqKgo9e/bEmTNnMG7cOKhUKixevFizXbt27fDHH39oXtvU4P7WCQmAlxeAjAyppkZERGQAFW7pXrx4MSZMmIDQ0FC0bdsWkZGRcHJyQlRUVKn7qFQq+Pj4aBZvb+8qBV1daObq5rRhRERkZhW9P+/Zswe9evXCk08+CX9/f/Tr1w+jR48u0TpuY2OjdQ/39PQ0xemYhSbpPnIE6NzZ3OEQEVENUaGkOycnBwcPHkRwcHDhAaysEBwcjL1795a6X3p6Opo0aQI/Pz8MGTIEx48fr3zE1Ygm6a5XD7h1y9zhEBFRLVWZ+3PPnj1x8OBBTZJ94cIFbNmyBYMGDdLa7uzZs2jYsCGaNm2KMWPGIDY2tsxYsrOzkZqaqrVYCk3SfbeIGhERkSFUKOlOTExEfn5+iZZqb29vxMXF6dynVatWiIqKws8//4xvv/0WarUaPXv2xNWrV0v9HEu5Ybu5saWbiIjMrzL35yeffBILFixA7969YWtri2bNmuGBBx7AzJkzNdsEBgZi5cqV2Lp1K5YtW4aLFy/ivvvuQ1paWqmxREREwNXVVbP4+fkZ5iRNQCvp7tLF3OEQEVENYfTq5UFBQQgJCUFAQADuv/9+bNy4EV5eXvj8889L3cdSbtju7kByMiTpZks3ERFZkJ07d2LRokX47LPPcOjQIWzcuBGbN2/Gm2++qdlm4MCBGDFiBDp27Ij+/ftjy5YtSE5Oxvfff1/qccPDw5GSkqJZrly5YorTMQhN0p2YCNTgbvRERGRaFaqG4unpCWtra8THx2utj4+Ph4+eFT5tbW3RuXNnnDt3rtRtwsPDERYWpnmdmppaLRNvd3fgwgUAjd2Aa9fMHQ4REdVSlbk/z5kzB08//TSeffZZAECHDh2QkZGB5557DrNmzYKVVcnn8m5ubmjZsmWZ93B7e3vY29tX4WzMJyEB8HJIA1xczB0KERHVIBVq6bazs0OXLl0QHR2tWadWqxEdHY2goCC9jpGfn4///vsPDRo0KHUbe3t7uLi4aC3VkWZMt6srkJJi7nCIiKiWqsz9OTMzs0RibW1tDQBQFEXnPunp6Th//nyZ93BLlpAAeF07wq7lRERkUBWe9yMsLAxjx45F165d0b17dyxZsgQZGRkIDQ0FAISEhMDX1xcREREAgAULFqBHjx5o3rw5kpOT8d577+Hy5cuaJ+uWTCvpTk42dzhERFSLVfT+PHjwYCxevBidO3dGYGAgzp07hzlz5mDw4MGa5PvVV1/F4MGD0aRJE1y/fh3z5s2DtbU1Ro8ebbbzNKa0NKDuxaNA+/bmDoWIiGqQCifdI0eOREJCAubOnYu4uDgEBARg69atmuItsbGxWk/Ob9++jQkTJiAuLg7u7u7o0qUL9uzZg7Zt2xruLMxEU0jNzY0t3UREZFYVvT/Pnj0bKpUKs2fPxrVr1+Dl5YXBgwdj4cKFmm2uXr2K0aNHIykpCV5eXujduzf27dsHLy8vk5+fqahu3wI8u5k7DCIiqkFUSml9yKqR1NRUuLq6IiUlpVp1NVcU4NFHgV82ZANjxgAbNpg7JCIisjDV9R5nKJZ0foMHA780nwZMngw0b27ucIiIqJrT9x5n9OrlNZlKdfcP9vZAdrZZYyEiIqLK0zRB3L4t48eIiIgMhEl3FWkSbyIiIrJYqal3i5YnJ8uwMSIiIgNh0m0ozL6JiIgslmaObrUauFtIjoiIyBCYdFeRotztklb9h8YTERFRKTRJN+/nRERkYEy6q8jJCcjMNHcUREREVBWapJuIiMjAmHRXkWaubgcHICvL3OEQERFRJWiSbg4XIyIiA2PSXUWapNvVlXN1ExERWaiEBMDLNQewszN3KEREVMMw6a4id3cpdApX17t/ICIiIkuTkAB42aVwujAiIjI4Jt1VpGnpdnNjSzcREZGFSkgAvKxvMekmIiKDY9JdRexeTkREZPmSkwG3vEQm3UREZHBMuqvIzY1JNxERkaVTFMAq5Tbg4WHuUIiIqIZh0l1FWt3LOaabiIjIct2+zZZuIiIyOCbdVcTu5URERDUEk24iIjICJt1VpFW9nEk3ERGRxcnIAJycwKSbiIiMgkl3FWlybU4ZRkREZJESEgAvLwC3WL2ciIgMj0l3FVlZAWo1OGUYERGRhdIk3bdZSI2IiAyPSbehuLgw6SYiIrJAmqQ7JUV6rhERERkQk25DsbEB8vPNHQURERFVkCbpVhTpwkZERGRAvLMQERFRraaVdBMRERkYk24DsLcHsrIAqFTmDoWIiIgqSJN0ExERGQGTbgPQTBvGJ+REREQWh0k3EREZE5NuA3B3l4KnUKmYeBMREVmYpCSgntMdwNHR3KEQEVENxKTbANzc7ibddeoAGRnmDoeIiIgqQK0GrFNvc45uIiIyCibdBqBp6XZ15bRhRERElug2k24iIjIOJt0GoEm63dzuDu4mIiIii3LrFpNuIiIyCibdBuDhwZZuIiIiS5SdDdjZQW7kHh7mDoeIiGogJt0G4OEhD8iZdBMRkTktXboU/v7+cHBwQGBgIGJiYsrcfsmSJWjVqhUcHR3h5+eHadOmISsrq0rHtDSayuXsXk5EREbCpNsAOKabiIjMbd26dQgLC8O8efNw6NAhdOrUCf3798fNmzd1br9mzRrMmDED8+bNw8mTJ7FixQqsW7cOM2fOrPQxLVFCAlC/Pph0ExGR0TDpNgBNSzfHdBMRkZksXrwYEyZMQGhoKNq2bYvIyEg4OTkhKipK5/Z79uxBr1698OSTT8Lf3x/9+vXD6NGjtVqyK3pMS8SWbiIiMjYm3QagaeBmSzcREZlBTk4ODh48iODgYM06KysrBAcHY+/evTr36dmzJw4ePKhJsi9cuIAtW7Zg0KBBlT6mJdIk3SykRkRERmJj7gBqAisrmeOTSTcREZlDYmIi8vPz4e3trbXe29sbp06d0rnPk08+icTERPTu3RuKoiAvLw8vvPCCpnt5ZY4JANnZ2cjOzta8Tk1NrexpmURiItCuHVhIjYiIjIYt3YbE7uVERGQhdu7ciUWLFuGzzz7DoUOHsHHjRmzevBlvvvlmlY4bEREBV1dXzeLn52egiI0jMfFuS3daGlC3rrnDISKiGogt3YbElm4iIjIDT09PWFtbIz4+Xmt9fHw8fHx8dO4zZ84cPP3003j22WcBAB06dEBGRgaee+45zJo1q1LHBIDw8HCEhYVpXqemplbrxDsxEfD0BKAo0nWNiIjIwHh3MSCljrM8KSciIjIhOzs7dOnSBdHR0Zp1arUa0dHRCAoK0rlPZmYmrIolmdbW1gAARVEqdUwAsLe3h4uLi9ZSnSUmAvXqQZJuIiIiI2BLt4E4OwMZd6zgzJs2ERGZQVhYGMaOHYuuXbuie/fuWLJkCTIyMhAaGgoACAkJga+vLyIiIgAAgwcPxuLFi9G5c2cEBgbi3LlzmDNnDgYPHqxJvss7Zk2QnQ04OJg7CiIiqsmYdBtIwbRhziqVuUMhIqJaaOTIkUhISMDcuXMRFxeHgIAAbN26VVMILTY2Vqtle/bs2VCpVJg9ezauXbsGLy8vDB48GAsXLtT7mDUGH5gTEZERqRSl+t9pUlNT4erqipSUlGrbTW32bGDECKDTnEeBTZvMHQ4REVkIS7jHVUV1P7/Bg4Ff1mYA48cDa9eaOxwiIrIg+t7jOKbbQApauomIiMgyaJodbt/mHN1ERGQ0TLoNRJN0W1kB+fnmDoeIiIjKkZ4OuLiASTcRERkVk24D8fCQezbq1mUFcyIiIgugqVx+6xaTbiIiMppKJd1Lly6Fv78/HBwcEBgYiJiYGL32W7t2LVQqFYYOHVqZj63W3N3vtnRzrm4iIiKLoJmjmy3dRERkRBVOutetW4ewsDDMmzcPhw4dQqdOndC/f3/cvHmzzP0uXbqEV199Fffdd1+lg63ONN3L3dyA5GQzR0NERETlSUoqknR7eJg7HCIiqqEqnHQvXrwYEyZMQGhoKNq2bYvIyEg4OTkhKiqq1H3y8/MxZswYvPHGG2jatGmVAq6uNN3L2dJNRERkETTdy9nSTURERlShpDsnJwcHDx5EcHBw4QGsrBAcHIy9e/eWut+CBQtQv359jB8/vvKRVnPsXk5ERGRZ2L2ciIhMwaYiGycmJiI/Px/e3t5a6729vXHq1Cmd++zevRsrVqzAkSNH9P6c7OxsZGdna16npqZWJEyzcHAAsrIg3cuZdBMREVV7mu7lLKRGRERGZNTq5WlpaXj66aexfPlyeHp66r1fREQEXF1dNYufn58RozQclQrS0s0x3URERNUeu5cTEZEpVKil29PTE9bW1oiPj9daHx8fDx8fnxLbnz9/HpcuXcLgwYM169RqtXywjQ1Onz6NZs2aldgvPDwcYWFhmtepqakWkXgrCti9nIiIyEJoku70dJnyk4iIyAgqlHTb2dmhS5cuiI6O1kz7pVarER0djSlTppTYvnXr1vjvv/+01s2ePRtpaWn46KOPSk2k7e3tYW9vX5HQqg92LyciIrII2dmAo+PdFyqVWWMhIqKaq0JJNwCEhYVh7Nix6Nq1K7p3744lS5YgIyMDoaGhAICQkBD4+voiIiICDg4OaN++vdb+bm5uAFBifU1gYwPkOrnClt3LiYiIqj1FKf4HIiIiw6tw0j1y5EgkJCRg7ty5iIuLQ0BAALZu3aoprhYbGwsrK6MOFa+23N2BZMUVXmzpJiIiIiIiIlQi6QaAKVOm6OxODgA7d+4sc9+VK1dW5iMtgocHcOuOI7zu3DF3KERERFQGRbnbo5yt3EREZGS1s0naSDw8gFu3OSaMiIiouktPB5ydwSJqRERkdEy6DcjdXWYdISIiouotMfHuHN2cLoyIiIyMSbcBeXgAt26ZOwoiIiIqD5NuIiIyFSbdBsSkm4iIyDIkJd2do/vWLSbdRERkVEy6DcjD4273chsbIDfX3OEQERFRKdjSTUREpsKk24Dc3e+2dLu6Apw2jIiIqNrSSro9PMwdDhER1WBMug1I073czY1JNxERUTWm6V5++7bct4mIiIyESbcBubgAqamQlu7kZHOHQ0RERKXQtHSnpsp9m4iIyEiYdBuQlRWgVoPdy4mIiKq5xMS7Ld2pqfLUnIiIyEiYdBsDu5cTERFVa1lZgKMjgLQ0oG5dc4dDREQ1GJNuA1OpwJZuIiIiS8GWbiIiMjIm3QamKIDi0wC4etXcoRAREVF5srIAe3tzR0FERDUYk24Dq1sXSG/UGjh1ytyhEBERkQ6KcrdnWgGtF0RERIbFpNvAPDyA23Bn9XIiIjK5pUuXwt/fHw4ODggMDERMTEyp2z7wwANQqVQllocfflizzbhx40q8P2DAAFOcilGlpwPOzuaOgoiIagsbcwdQ07i7y1zdjQEpZW7F5xpERGR869atQ1hYGCIjIxEYGIglS5agf//+OH36NOrXr19i+40bNyInJ0fzOikpCZ06dcKIESO0thswYAC++uorzWv7GtAVWzNdGBERkQkwIzQwDw9JuuHnB1y7Zu5wiIiolli8eDEmTJiA0NBQtG3bFpGRkXByckJUVJTO7T08PODj46NZtm/fDicnpxJJt729vdZ27u7upjgdo0pKKpJ0s2s5EREZGZNuA/PwAG7fBtC6NXDypLnDISKiWiAnJwcHDx5EcHCwZp2VlRWCg4Oxd+9evY6xYsUKjBo1CnXq1NFav3PnTtSvXx+tWrXCxIkTkZSUVOZxsrOzkZqaqrVUN5o5uhXF3KEQEVEtwKTbwAq6l6NNGxZTIyIik0hMTER+fj68vb211nt7eyMuLq7c/WNiYnDs2DE8++yzWusHDBiAr7/+GtHR0XjnnXewa9cuDBw4EPn5+aUeKyIiAq6urprFz8+vcidlRJru5ZrJuomIiIyHSbeBabqXs6WbiIgsxIoVK9ChQwd0795da/2oUaPw6KOPokOHDhg6dCh+/fVX/PPPP9i5c2epxwoPD0dKSopmuXLlipGjrzhN0p2aKtOOEBERGRGTbgPTdC9v1IhzdRMRkUl4enrC2toa8fHxWuvj4+Ph4+NT5r4ZGRlYu3Ytxo8fX+7nNG3aFJ6enjh37lyp29jb28PFxUVrqW6Sku52L09NBaphfEREVLMw6TYwTUu3lRXHihERkUnY2dmhS5cuiI6O1qxTq9WIjo5GUFBQmfuuX78e2dnZeOqpp8r9nKtXryIpKQkNGjSocszmpGnpTktjSzcRERkdk24D04zpBgA3t7vN3kRERMYVFhaG5cuXY9WqVTh58iQmTpyIjIwMhIaGAgBCQkIQHh5eYr8VK1Zg6NChqFevntb69PR0vPbaa9i3bx8uXbqE6OhoDBkyBM2bN0f//v1Nck7GoimkxpZuIiIyAc7TbWD29oBm2tPWraWYWjmtDERERFU1cuRIJCQkYO7cuYiLi0NAQAC2bt2qKa4WGxsLKyvtZ+2nT5/G7t278fvvv5c4nrW1NY4ePYpVq1YhOTkZDRs2RL9+/fDmm29a/FzdmvppbOkmIiITYNJtTAUVzJl0ExGRCUyZMgVTpkzR+Z6u4metWrWCUspQKEdHR2zbts2Q4VU/bOkmIiITYPdyY2IFcyIiouorLY1JNxERGR2TbmNq3hwoo8IrERERmZZWwz6nDCMiIhNg0m0ENjZAbi6KDfAmIiIic0tPL5Jns6WbiIhMgEm3Ebi7FylabmcHZGebNR4iIiISmunCALZ0ExGRSTDpNgIPjyJJd4sWwNmzZo2HiIiIRFLS3enCABZSIyIik2DSbQQeHkXm6i6YNoyIiIjMTqulm1OGERGRCTDpNgJ39yJJd5s2rGBORERUTWgl3dnZUn+FiIjIiJh0G4FW93K2dBMREVUbWt3LVSqzxkJERLUDk24j6NoViIoCfv0VgJsbkJxs5oiIiIgIkKTbw8PcURARUW3CpNsImjaVhHvvXmDUKOBKlhegVps7LCIiolqPs4QREZGpMek2EicnYOFCYO5c4IVzr2LrNwnmDomIiKjWy8gAnJ3NHQUREdUmTLqNrG1bIGrCXqxbnWfuUIiIiGq99HSgTh0AimLuUIiIqJZg0m0C3j3uwc0r2eYOg4iIqNbLyJDeaMjMvPsHIiIi42LSbQo9e8IxJQ537pg7ECIiotpNrQasrQGkpnJwNxERmQSTblNwckJH9ys4ujvV3JEQERHVappZwtLSgLp1zRoLERHVDky6TeTefp44uJrzdRMREVULbOkmIiITqVTSvXTpUvj7+8PBwQGBgYGIiYkpdduNGzeia9eucHNzQ506dRAQEIBvvvmm0gFbqi7Pdsahv9LNHQYREVGtpqmfxrnDiIjIRCqcdK9btw5hYWGYN28eDh06hE6dOqF///64efOmzu09PDwwa9Ys7N27F0ePHkVoaChCQ0Oxbdu2KgdvSRq088CNjLrgwG4iIqJqIDWV3cuJiMgkKpx0L168GBMmTEBoaCjatm2LyMhIODk5ISoqSuf2DzzwAIYNG4Y2bdqgWbNmmDp1Kjp27Ijdu3dXOXhLY+/jjqwtO8wdBhERUa2lNaabLd1ERGQCFUq6c3JycPDgQQQHBxcewMoKwcHB2Lt3b7n7K4qC6OhonD59Gn369Kl4tBauwwOeOPb1QXOHQUREVCvl5gI2NndfsKWbiIhMxKb8TQolJiYiPz8f3t7eWuu9vb1x6lTpRcJSUlLg6+uL7OxsWFtb47PPPkPfvn1L3T47OxvZ2YXzWqem1oyq310ecsPBcDd0zc+/O18JERERmUpGBlCnzt0XbOkmIiITMUn18rp16+LIkSP4559/sHDhQoSFhWHnzp2lbh8REQFXV1fN4ufnZ4owja5LF+CgY29gzx5zh1LtJCQApZQFICIiMoj0dMDZ+e4LtnQTEZGJVCjp9vT0hLW1NeLj47XWx8fHw8fHp/QPsbJC8+bNERAQgFdeeQWPP/44IiIiSt0+PDwcKSkpmuXKlSsVCbPaatgQuObUHPjxR3OHUu2sWQOsX2/uKIiIqCbLyCiWdLOlm4iITKBCSbednR26dOmC6OhozTq1Wo3o6GgEBQXpfRy1Wq3Vfbw4e3t7uLi4aC01gUoF2NWri5x/TxaZs4QA4PRp4OpVc0dBREQ1mVZLN7uXExGRiVRoTDcAhIWFYezYsejatSu6d++OJUuWICMjA6GhoQCAkJAQ+Pr6alqyIyIi0LVrVzRr1gzZ2dnYsmULvvnmGyxbtsywZ2Ih2rdX4djFvrj3v/+Ajh3NHU61ceEC4Olp7iiIiKgmS08vMqab3cuJiMhEKpx0jxw5EgkJCZg7dy7i4uIQEBCArVu3aoqrxcbGwsqqsAE9IyMDkyZNwtWrV+Ho6IjWrVvj22+/xciRIw13Fhbk3nuBgzmP4t7vVxo16X7hBeDVV4HmzY32EQZlbQ2kpJg7CiIiqsm0Wrrz8gBbW7PGQ0REtUOlCqlNmTIFly9fRnZ2Nvbv34/AwEDNezt37sTKlSs1r9966y2cPXsWd+7cwa1bt7Bnz55am3ADUkztUEozICZG7v5GkJQEXLwIvPQS8P33RvkIgyr4EsQe90REVbN06VL4+/vDwcEBgYGBiImJKXXbBx54ACqVqsTy8MMPa7ZRFAVz585FgwYN4OjoiODgYJw9e9YUp2IUWmO6iYiITMQk1cupkJ8fEHtFBTz/PPDFF0b5jH//Bfr1A37+GTh0CJg8GcjKMspHlSovT/9tz5wBWraUMe9qtfFiIiKqydatW4ewsDDMmzcPhw4dQqdOndC/f3/cLGVqiI0bN+LGjRua5dixY7C2tsaIESM027z77rv4+OOPERkZif3796NOnTro378/skx9UzEQrZZuIiIiE2HSbWIqFWBjA+Q+MgzYvBkoo6BcZR0+DAQESK+5t98GHn4YMHXngnnzgL//1m/bgqS7fn2ZOoyIiCpu8eLFmDBhAkJDQ9G2bVtERkbCyckJUVFROrf38PCAj4+PZtm+fTucnJw0SbeiKFiyZAlmz56NIUOGoGPHjvj6669x/fp1/PTTTyY8M8PRGtNNRERkIky6zaBdO+D4SSvg6aeBr782+PGPHJGku8CgQfIl4/Ztg39UqY4dA06e1G/b06eBVq2kF0ANmR2OiMikcnJycPDgQQQHB2vWWVlZITg4GHv37tXrGCtWrMCoUaNQ525WevHiRcTFxWkd09XVFYGBgXofs7phSzcREZkDk24z6NIFOHgQwJNPAt99V7G+2Hq4dQuoV097XbduwIEDBv2YMl26JC3Y+mDSTURUNYmJicjPz9cUNS3g7e2NuLi4cvePiYnBsWPH8Oyzz2rWFexX0WNmZ2cjNTVVa6kuNGO6OZaJiIhMiEm3GTz4ILB2LfDLNjtg2DBgwwaDHfvOHcDevuT67t2B/fsN9jFlys0FGjYEzp/Xb/vUVMDVlUk3EZG5rFixAh06dED37t2rfKyIiAi4urpqFj8/PwNEaBialm5WVCMiIhNi0m0GHh7AL78AO3YAL554AXc+/9pgpbuPHwc6dCi5/t57paiaKVy8CLRuDeTklL9t0dNu1IhJNxFRZXh6esLa2hrx8fFa6+Pj4+Hj41PmvhkZGVi7di3Gjx+vtb5gv4oeMzw8HCkpKZrlSjX6h12TdKelAS4u5g6HiIhqCSbdZuLgAHz4ITDoUVsMvrIU/y750yDHLSiiVpyjo1QwN8W0XAXdxa2sgPz8sre9fl1axQFp6b561fjxERHVNHZ2dujSpQuio6M169RqNaKjoxEUFFTmvuvXr0d2djaeeuoprfX33HMPfHx8tI6ZmpqK/fv3l3lMe3t7uLi4aC3VhaaQWmoqULeuucMhIqJagkm3mQ0cCKze6okPP7XFs0134MqG/VXKjI8cATp31v1ekyZAbGylD623gqS7cePyP69gW0C+/xhp6nIiohovLCwMy5cvx6pVq3Dy5ElMnDgRGRkZCA0NBQCEhIQgPDy8xH4rVqzA0KFDUa9YMRCVSoWXX34Zb731FjZt2oT//vsPISEhaNiwIYYOHWqKUzK4O3fkITRbuomIyJRszB0AAd7N62Ll+ftw5PebmPpiAlq8th4zFteH+7AHKnysS5ckudalYFx3ae8byunTUiOuZUsppnbPPWVvW5B0ExFR5Y0cORIJCQmYO3cu4uLiEBAQgK1bt2oKocXGxsLKSvtZ++nTp7F79278/vvvOo/5+uuvIyMjA8899xySk5PRu3dvbN26FQ4ODkY/H2NRqcCWbiIiMikm3dVIQL/62Hi6PnZsaYaRTx/Hb4MVWNuo9N4/P1++TKhK2SUwEIiKAp54wkABl+LGDaBBA0m6z54F+vcvfdvTp4Eis9FApZLzsLY2boxERDXRlClTMGXKFJ3v7dy5s8S6Vq1aQSmjd5VKpcKCBQuwYMECQ4VYPbClm4iITIjdy6uh/w1ywNAecVgx42yF9jt/HmjRovT3W7UCTp2qYnB6UqkKW7rLcvGidku4jw+gx+w2REREFaZ5vsCWbiIiMiEm3dXU8x+3w/o1uUhK0n+f0oqoFbC2lmTYwNOCa0lOlum/ABnTffly2dvn5QE2RfpbsJgaEREZHVu6iYjIhJh0V1PWzfzxRrOvMWd6tt77HDlSdtINAO3bA8eOVSm0MhUdo21tXXb18qysknOKc65uIiIyFs3wq9RUJt1ERGQyTLqrsZ6TApB96iIOHtRv+xMngDZtyt4mMBCIiSm5vrypvfRVvDCavT2QXcpzg3PngObNtdcx6dbfqVMy5RrVLllZ5o6AyDJpDV1n93IiIjIhJt3V2bBhWGT3BmbPBtTq8jfPzQXs7MrepqCCeVHJyTL++vbtSkeqUTzpbtoUuHBBv20BJt0V8fnnwE8/mTsKMqWkJMDfv/TfKSIqXU5OkXsku5cTEZEJMemuzhwc4N3OEwM63cCnn5adeMfFScXw8jRsKNXFiwoPB/73P2DLlqqFC0i18qLF3MoqpnbmTMmku1EjJt36OnQIOHrU3FGQod25U/p7S5cCb7wBTJ2q34M4IiqUng44O999wZZuIiIyISbd1d0zz2By1gdISQEGDABefBH4640/kf/oMHlsf1d5RdSKcnaWh/wA8Ndf8v+ICOCXX0rf56mnyi+KBgCZmUCdOoWvC6YN00VXS7eTU9lJB4nMTMDTkw8oapozZ+ShVWpqyfcyM4Hdu4HnnpNp+CIjTR8fkSXLyCiSdKelMekmIiKT4Tzd1V3nzrCZMQNzfskBXsnDydB3sf5IL8y//TlmjI5Cvx9eACBF1Hr31u+QXbsCBw8CPXpIq9nGjVJxPD1dxos6OGhvf+MGsGcP8P33wGuvlX7c/HzAqthjnJYtgdWrdW+fmCiJY3GlzTNOhf75R4YK/PWXjFOsDtcsNxcYPRrYsMHckViuDz4Apk8H5s8HFi/Wfu+rr4DQUPm7njQJePRRYOBA7Sn3qrv8fOmVk5kpyY+zszykUxQZ3pKUJP8u3LwpsxgULEUfQqhUVfuZL7p/wRhfXccr7/2NG7VnXqDqT6ulOz+ff4FERGQyvONYgsGDgXfeAXbuRJsZMzC3b1+8kgHMva8VvnsoFu+ta4x//wUmT9bvcAXjunfsAKZMKZziKzhY1g0apL39t98CH34ILFtWdtIdGyvThBXl41OyOzsgX2JL++JsbV1yKjHStmePPGSJjZUeCP7+5o5ICvn9+CNw6xbg4WHuaCxPXJwknC++KD1Ljh2T2QYA+X3YuBHYtk1eW1kBH30EvPQS8PPPJR92VURFE1hFkToQCQmFS3y8/J7HxcmfixdmLEhebWwAb2/p0ZKRIUlQRoa85+4O1KsnD+K8vKQXzEMPyZATDr0lQ9BKuomIiEyIaY0lGDMGmDYNWLNGvrFCWoc+2NsTB/qEYfSw95GW66j3F9OuXYFXXpFurAsWFK4fMkS6mRdNuhUF+P13+fhffwXOnweaNdN9XF3dxYu2FhWVmChfrHVp2FCqchdP4KnQP/8AL78MHD8u47qrQ9J9+LAkSTt2AI8/bu5ojOPsWWl17dLF8Mf+6CMZqw0Ab78NTJwIbNokv0M//AAMHar9IKpZM6BvX+CLL4AXXtD/c1JTgb17gb//lr+zvDxJ2q2t5Zht28rnpKVJkpKWVvhAoIC7u/z+enrK0rAhcO+9UlfC2xuwtTXIJSEyqPR07eFPREREpsKk2xK4uwMrV5Zcb2+Prt+9gi0TR+D4Wz8C0O+brrOztB4V7756zz3SapqfL1/AASnWFRAgX8KfeAJYvx6YMUP3cU+f1j1lmbNzyRYGXQl6gYIK5ky6dVOrZRo2R0egY0dJch991NxRSQL3+uvyM1ITk+47d4Dnn5ek8rvvDHvs1FQZIrJokbxu1Eh6MqxbB4wcCURFSUt3cVOmAOPGSY+HmTNLb8XLzZVW8tWrZQjJ/fcDjzwCzJlTmCDn5UlV9JMn5d+ARo1kyGvdutJjxcOjegxjIKosrTHdREREJsRCapauaVPYThiHgKiXZKCkLgXZUJEJfn/+WVqniuvRQ3tKsVWrgLFj5c8PPgj8+WfpoZSWSOsqphYTA7Rrp/s4jRrJOM7abt8+6bpbXNHr3L69dEOuDs6flyr4+hTcM5S4ONNV8X7lFfk1Sksrfe75yvriCymQVjSpnTYNWL5cfleDgnS30FlZAV9/LQn6kCHyMKCgZ8m1a1KHYcoU4OGHpUjbhx/KEICXX5ZhJkVbpG1s5Hd1yBDgscekFb1HD/k9rVePCTdZPnYvJyIic2HSXRM8/rh8Qx46VJrKUlJk/cWLQEiIDMZu3RoIC9O9f06ONJOlpWHoUPmSX7D6zJnCcaU2NtIaXtoUYFeuSCt1cS1aaO+TkiLTkxUfO16gpszVfetW1fafOxf49NOS6/fsAXr1kj+7uBRWoi8uL0+rwL1RFSS+VlYyN/v588b/zLw8+bH//HPjf9b69dLiO2CAPFjYscNwx87JAX77TZLdouzspKja+PHl12sYNEiOcfUq8MAD8nr+fGnZe+klGSISFiYt1kS1lSbpVqurVgiBiIiognjXqSkee0z6j3bsCIwaJX3BZ8yQSbi/+AJ45hnpH/r119r75edLSWQAmD4dAQHSMK4ouhPjkSMlAdFFUXR/jyk+V/eiRZJMlFYorSYk3YoiXXg//rhy+yclAW5uMu42N1f7vb//Bnr2LHxtb6/ViUEjKkqS84sXdX/G6dPyd20IFy5Isg1IIrx9u2GOW5ZVqyQZ/eUX4/aMuHgRWLECeOsteT1kCPDTT4Y7/urVUvVd1+9Ov35Sob60+gdF2dlJocM//5Tf3eXL5Ve7ZUvDxUpkyTRjutnkTUREJsakuyZRqWSg5pYt0o903TrtQdZvvCHzOf33n7xWq6VP6yOPSCZsbQ1V9B9o3Ro4dUrqto0erf0RffoAu3aV/OiyCtS0aFHYvfzCBWkF7du39NMwRvfymzdlmiVTdUU+ckQSpuPHS39IUZZffpHkbujQkgleXJwUrCrQtq1UDi9u2zaZZmrCBO1hAbm5UjBv+nQp2GUIhw8DnTvLnx980LAtwbpkZ0uyOn68TLP18su6C/ZVVW6uFDRbtqywK3azZsClS4b5WUpLk9kBnn669G1KG4ZRGjbgEemmGdOdmso5uomIyKT49awmUqkAX9+S662tgS+/lBLJKSny/549CzPriAhg0SIM7ZuBL7+UVcVb2KytpeXs5Ent9WfOlN6i5uFR2NV61ixg4cKyw7e3N/yY2RUrJIZ33zXscUvz3XdyWZcuBdau1f2goiy//irjcENCtDsnJCbK+NqiOnYsfI5SICNDksL27WW4wJdfSlf1w4el90KjRjK2NyPDMF3Qiybdbm7yECYvr2LH2LFDKutHR5e/7eefS8JtayvPlTp1kmdMhqQoMn3XhAkl58IODJS6BOVJTZXrfOdOyfc2bZIHK/Pmyc88ERmXpoE7LY3z0BERkUkx6a5t6teXrLdrV6B5c8lcCjg7A/Pm4b5fXsfXX5ds5S4wcqQUaCqqrGrkgCQwu3bJmNKytjOG/HxJ5NaskRZhXQlwaqru6tCVoVZL1fcuXaQL/ddfy/MMfQuepaVJzG5u8r2wUaPCluy9e7W7lgOSdB89qr1u+/bC3gR16khraloasGRJYcuqSiXdz/fsqcrZihMnpMW9QLduwIED+u2rVgNvvgl88w2wdSvwyScyBrk0GRmSsI4aVbhu+nR5sJCUVLn4dZk/X0ohDB9e8j1dPRCKy8mR+bZPnpTRHy+9JH9P167J79b+/cDmzdJ7hIiMT5N0s6WbiIhMjEl3bRQUJFlZwaTARd1/P2wdbbBk/FE8/LDu3Xv1knHFcXHA7t3AqpUKVkXlaSVdxXl5Sa22uXP1C9Eu8TpyPv9Kv40h3dF/+UX3e1u2AAMHSrfbzz6TXvbx8YXvHzoEDBsGREZKd/CqKkiMC6o916kjCeVLL8l3vfL89pv2WPqJEyVuQBLk4kl38+Ylq8P/8gsweHDha5VKhvevWqWZ6h2AXJffftP/3EqTmytjigvoO647IUES0oYNZQy6p6f0Eli2TBJwXT7+WCpyF0xrB8hnL1oEvPpq2Z+3erUcu7xW+MhIGSf/8su63+/cWX5uSqNWSwv5hAnyc//bb8Czz8qDgUmTCnt8ODqWHQcRGY5mGBRbuomIyMQ4T3dt5e9f+nuLFmHMoEHAE+6SPRQMlr2bRVoBGBHXF7O7t0LzOjfQ3Ok63rGPRaddLYF7p+k8ZPv2MkWRu7sesZ07B9+4E7i2YS/ueaBnuU3jWVnSYJ+TI8lQo0ba73/1lRSVAqSV4+OPJQH68UdJwP76S1ru09OlwvMPP+gRYxm++65ktWkvL+CFFyT5Lq8S9c8/y5D8Ah07SjGvtDTpRl5Q0KuAjY20jBfIz5fWVF2V5Ivr1Kn0edf1deOG9hhzQLpflzdePDdXek18+KHEUcDRUa7hk0/Kj1/RBxDJydJTQVfM3bvL+PnNm6HzgdH165J0P/aYVCF/9VX5f3E//ihdx1esKD12lUp+LE+dktbw4sLD5eFI0QcfHTtWvrAeEVWd1phuJt1ERGRCTLqppDp1gD/+kKbhos2JRUy4u2goipRKLtqvuYiwMD0LPGVlARMnwu/J9bhybw/cMzUEFz/5Fb9utUG9epKIFf/Yl16SRvsmTSSRWru28P3z56WbdtFx0O3byyxrnTtLUffvv5ckql49KZK1Y4dMC1UZeXnS6ly0fl2BYcOkZXnSpNLnPM7KkuH29etrr3/qKXl4YG2t+6/Ey0uKxdWvLwljYKB+8RYM/y9tujd9FB3PXcDWFnBwkAcFpfXi/Ppr6SJeNOEu4OAgife0afJgRFHkgU1iosyXXdr1e/NNSbgDA6XVvKjXXwfeeQfo0EGS/bfflrHhDzwg1z0rSx68XLggP0PlzUtdML1e8aT700+l5f3558ven4hMKzv7bo+csv5hIiIiMgJ2LyfdbG1LTbh1UqkkO3r/feDcuRJv611R+bXXgKlT4dfBDYu+rI/+11bgnaf+08wP/txz2kWpPv8caNxYWkPbtZNpq379tfD9yEjpnl3c2LEybnjaNO3kauZMScbKq0ydlAQEB5ecs7yshN3WVsbvllUoLDpajlvcY49JUbbiyW2BosXUNm0CHn207PiLGjCg9K7c+jh0SHdc999fegG5nBxpdR43rvTj2ttLt/pffpHl44+Bjz7SfX0KODhIsbyXXtKuZv7771JPoEMHeV23rnTv/vhjWde7txTxHztWxv4XVCovS/FK/qmpktifOAEsWFD+/kRkeioV2NJNREQmx6SbDMfRUfrkPv+8foOXi9uwQTKtRx7Bww9Lq+TWfxsiskkEHmnyH+bPl+RzyBBpjdyzR6bCmjmz8BCzZwOLF0s3wjt3JBHt1k33xxXvEg1Iq/igQdLKWprcXEkWX31VuowXnVN87VrtAl/FPfecTJtemo0bpUW8OHt7aZUvbaq1osXUDh0C7r239M8orqrzah89Kp9fkeOuWiW9FoqOAy+LSiUt3S1blt8C3aWL9DRYs0ZeZ2XJz9L8+SW39fOThyS9esl+7dvrX0nc1lZa069dkx/7YcPknD/7rPwYiciM2NJNREQmxu7lZFiNGkkz3/jxMoDZwUG//c6elSpTd6uhubgUdDtWSXNkSAiweTMGDLBF27ZSoCo7W1p1i7aiOzlJMrxggVTTHjmy4qcwaZJ0Ax8+XHf4L78sPekHDJAkcNw4SbZdXKRAW5MmpR/bx0eOeelSyWH1eXmSwJW2//TppR+3QwdJMs+fl+mtKpL0ubkVTh1WNAlWFDkfe3u5rnZ2uo+rGSdZTKtWUrm7+LjnnBx5qLFtm/4xVlR4uLRc33+/JMSTJ+uOsaqGDAEeekgKu23bJuPriaiaS0goOfciERGREfErIhler15S2vyppyQzVhQZJ/7cc5KhFPfffzIo++uvdffr9fGRZt533wVmzULjxpJsl1aAdtAgmRbrzz+BnTsrHn7BeNwlS0oW7PrsMwnnscfkddOm0uV5zBiZhmvgwPKPP3GidHsvXmjs//6v8tNHeXtLgly8arm+CqYOe+CBwnVLlkiROReXwqRcUaRYWcHDiORkwNVV9zFVKvkrffJJOd+CedxXrpQfDX26cFeWjY38vYSESAu5vlXzK2rYMKBfPzaaEVmUa9dKVtwkIiIyIibdZBzDh2tPcJySIgNeo6Nlzq6CjOvvv+X1unVSDaw0I0fKQOXr14GGDWFvX3Y34PdfOIff3/sXTk46JlnWw4gR0rLcv78kiwMHSsL599+S0BfVvr20rD/yiH5TjvXoIV2d79wpnDLqyhWZ8qpgarDKsLaW7tyVKeA1cKC01hck3Vu2yLOQjRu1W7e3bJHx0p9/LuuPHAECAko/ro+PXK+nnpIK8n5+8lddlTHk+mrVSlqg773XeN29rayYcBNZnKL/+BIREZkAx3STabi6SqbWsaMkz5cuydxO77wjc3SVlXADkjUtWKBfk2VODhq+NQnjfLdLta5KUKmkYX3rVuDFF6X3+y+/SOKoK4Hr0UOm9So6B3ZZxx41SpJPRZGk9Nlnpep1ixaVCheAzNdtZ1e575IBAZJAA/Lg4NNPpS5e8XMdNEgK133+ubzWVbm8uIYNpcV7wgRg3jzpEWDMVu6iHnus7NnxiKh2KFpYkYiIyNTY0k2mNWqUTKj8wgsypm79ev0rV3XuLFngwYNS9ao0CxZIhjdsmCT4PXrIXGCVoFJJS3dB1+iyVKTFc9QoaRnftk2qrm/eXPXxwPfdJ6daGQVThx05ItO7rV1b+l/LzJnA6NHy/OTwYUmiy9OokXQrf+01mdaLiMiUsrLuPpDMz6/AdBpERESGwTsPmV7TptKEvHq1/gl3gYLW7tKaLf7+W8brjRghWeyyZVIZLSen6nEbkKOjtG6/+qpUXDdEAa5hw4Annqj8/gMGyDOKJUtKzhNelJWVtPjPmSMdForPh12axo2ldZ/FxoiMZ+nSpfD394eDgwMCAwMRExNT5vbJycmYPHkyGjRoAHt7e7Rs2RJbtmzRvD9//nyoVCqtpXXxyektQHr63WKKN2/KuBciIiIT4tdfsiwNGkjVr40btceMAzJN2Zw5wI8/Fq5r0kQGOc+aBbz3nmljLcfo0eaOQNsjj8jzEF3TfxXn4iLjz1euNHpYRKSndevWISwsDJGRkQgMDMSSJUvQv39/nD59GvV1PEnLyclB3759Ub9+fWzYsAG+vr64fPky3NzctLZr164d/vjjD81rGwt8cqZJuq9elW49REREJsSWbrI806YBS5dKf8GiXnlFKpQVL6f92GNAZqZUASuNokjf6sxMQ0drMZycgK5d9d++VSsgIsJ48RBRxSxevBgTJkxAaGgo2rZti8jISDg5OSEqKkrn9lFRUbh16xZ++ukn9OrVC/7+/rj//vvRSeZr1LCxsYGPj49m8dS3e0s1opV0s3I5ERGZWKWS7op0X1u+fDnuu+8+uLu7w93dHcHBweV2dyMqk6OjdBl/6CHpD12wtG1b+pxb778vk0O/8orMNVbUiRPSzPvVV1Jme9AgGdj8888y5RkRUTWXk5ODgwcPIjg4WLPOysoKwcHB2Lt3r859Nm3ahKCgIEyePBne3t5o3749Fi1ahPz8fK3tzp49i4YNG6Jp06YYM2YMYmNjy4wlOzsbqampWou5ZWTIzJVMuomIyBwq3Eesot3Xdu7cidGjR6Nnz55wcHDAO++8g379+uH48ePwZRcvqqzHH5fu5frOBeXoCHzzDfD778CQIcDkyZJcv/UWcO6cTCTt5yfbqtXAqVMyvdnQocA99wDjxgHdupX9eXl50pySmSmLWq1fBTYioipKTExEfn4+vItNoeDt7Y1Tp07p3OfChQvYsWMHxowZgy1btuDcuXOYNGkScnNzMW/ePABAYGAgVq5ciVatWuHGjRt44403cN999+HYsWOoW0r1yIiICLzxxhuGPcEq0mrpfughc4dDRES1jEpRKjaRRmBgILp164ZPP/0UAKBWq+Hn54cXX3wRM2bMKHf//Px8uLu749NPP0VISIhen5mamgpXV1ekpKTAxcWlIuESlZSVJVOVbd8uc4c/+mjZ2588CaxaJVXTJ02SRLxo8p2XJ0n7xo3SguLkJEtSkoxBj4gw3kTRRGTxDHGPu379Onx9fbFnzx4EBQVp1r/++uvYtWsX9u/fX2Kfli1bIisrCxcvXoS1tTUA6aL+3nvv4caNGzo/Jzk5GU2aNMHixYsxfvx4ndtkZ2cju0gvodTUVPj5+Zn1Hv7TT8CtW8AzO56SghT8LkFERAag7z28Qt3LK9N9rbjMzEzk5ubCw8OjIh9NZDgODjJh9O7d5SfcANCmDfD229Ld/MQJ4OGHgX375L29e4GBA+WYf/whE1JHRgKLF0ui3rgx8NxzkpgTERmJp6cnrK2tER8fr7U+Pj4ePqVU627QoAFatmypSbgBoE2bNoiLi0NOKTM+uLm5oWXLljh37lypsdjb28PFxUVrMTdNS3dKChNuIiIyuQol3WV1X4uLi9PrGNOnT0fDhg21EvfiquN4MCI4OUkV9JUrpat6nz6SZK9dK/N/6Zr7ddIkoG9fKVV+547JQyai2sHOzg5dunRBdHS0Zp1arUZ0dLRWy3dRvXr1wrlz56BWqzXrzpw5gwYNGsDOzk7nPunp6Th//jwaNGhg2BMwMs2YbiIiIjMwafXyt99+G2vXrsWPP/4IBweHUreLiIiAq6urZvErGGtLVB3Ury/V03//XeYBr1ev7O2feEKmLRs+vGQRt7L884/0iTx0CEhIKH1uciIiAGFhYVi+fDlWrVqFkydPYuLEicjIyEBoaCgAICQkBOHh4ZrtJ06ciFu3bmHq1Kk4c+YMNm/ejEWLFmHy5MmabV599VXs2rULly5dwp49ezBs2DBYW1tjdHWb87Ac6emAcx3+G0pEROZRoUJqlem+VuD999/H22+/jT/++AMdy5kIODw8HGFhYZrXBePBiKqVMh4clRAcLMXcnnoK+P57wN6+9G3v3JEW9dRUICgIOHwYuHIFiIuTom7vvHO3n2QRiiIt7g0bAvffX7nzISKLNnLkSCQkJGDu3LmIi4tDQEAAtm7dqumdFhsbC6siPXL8/Pywbds2TJs2DR07doSvry+mTp2K6dOna7a5evUqRo8ejaSkJHh5eaF3797Yt28fvLy8TH5+VZGeDjjnpwAWON0ZERFZvgol3UW7rw0dOhRAYfe1KVOmlLrfu+++i4ULF2Lbtm3oqsdEwPb29rAvKykhskS9egETJgDjx0u3dF3d0fftA2bOlKnNHn645Ps7dsg49DlzgAcflHWHDgGzZwP/+x+wfr1UTh840LjnQkTV0pQpU0q9H+/cubPEuqCgIOwrqFGhw9q1aw0VmlmlpwPO6XEAZ00hIiIzqPCUYWFhYRg7diy6du2K7t27Y8mSJSW6r/n6+iIiIgIA8M4772Du3LlYs2YN/P39NWO/nZ2d4Vy8tY6opnvkEalq/vLLwEcfFVY1P3ZMKupmZgI//AC4u+ve/3//k6nLpk+Xaul5eTKX+IoVUik9OxsYM0bWDx5cehy3bwObN0urz4ABBj9NIqLqJCMDqJN8jXN0ExGRWVQ46a5o97Vly5YhJycHjz/+uNZx5s2bh/nz51cteiJLNHasjNGePx9o2hRYt07+//zzQKdO5e9ft64k6Lt2STf1Hj0K37O3B9asAZ5+WhLvYcNkfVYWcOaMVGzftg2ws5OW9G++kUR9yBCjnCoRUXWQng4434oFWjDpJiIi06vwPN3mwHm6qUb6+GOZuuaJJ6QyuiHl5kpyn54OqNWSjLdsCXTvDvTrV1jGNycHePJJ6fbev79hYyAivdT0e1x1OL/HHgPWtZ4H2yeGAQEBZomBiIhqHn3vcRVu6SYiA3npJeMd29YWWL1aku9Spv4BIO99+y0wcqQUeuvTx3gxERGZSV4eYBt3hd3LiYjILJh0E9VUKlXZCXcBBwfpkj5iBHD9OtCmjVRBr1dPd7E3IiILo1IBSEwsf4pHIiIiI2DSTUTS3XztWmn1Xr9eku/EROl+PmYMMGqUtJ4TEVkgRbn7n4LilURERCbEpJuIhIsLMGmS9ro7d2R6swEDgEGDpODa0aPA/v3A8eOSiK9bp1+LOhERERFRLcS+o0RUOkdHqar+++9SYX3xYpnybMwY4Oef5f+chYCIqru8XHmwSEREZAZs6Sai8llby/RjBVOQFXj8cWDrVpm+7P77zRMbEVE5VFlZgK+vucMgIqJaii3dRFQ1H34IvPEGkJysvT45GVi1Crh8Wb/jJCQAs2bJ2PLUVENHSUS1lFoNqLLusHI5ERGZDZNuIqqaunWBRYuAF1+UQkW5ucAnn0g19Px8YMECGQ/+4YfAlSsl91ergRUrpKv6Qw8BmZlAaCgwfDjw5ZfA3r1S2E2tNv25EZHFy8wEnNRpTLqJiMhs2L2ciKquRw9g2zbglVeAf/8FQkLktZUV8MwzUgV9+3YZ/339unTz7NMH8PcH3nkH6NcP2LIFsLn7T9IzzwApKTKWfMcOIDYWiIsD0tOBDz4AAgLMeLJEZEnS0wHnvBQm3UREZDZMuonIMGbNkkrmb70FODlpv2dnBzz8sCwAcPUq8H//J0n155/r/jLs6iqt5UUlJcm61auBBg2Mcx5EVKNkZADOObcA3zbmDoWIiGopJt1EZBg2NtJFXB+NGgGjR1f8M+rVA5YtA8aNA378sWRyT0RUTHo6UCf7FuDjY+5QiIioluKYbiKyLK1aATNmAOPHc5w3EZUrPR1wtsqUWRiIiIjMgC3dRGR5HnwQuHABCA+XVu9Ll2SJjQW6dAEeeQRwcDBzkERUHaSnA842d8wdBhER1WJMuonIMo0fL1XSv/pKCrL5+wM9ewJ79gCPPQb4+Ul3d0dH4NgxWc6cAUaNKr0bfEHLuRU7ARHVFBm3suHskG/uMIiIqBZj0k1EluvFF0uu69QJmDgROHsWWLNG1rVrB0yYANxzD/Dqq9IiPmMGoFIV7nfuHDBpEtC0qYwbL/oeEVms9GvJcK7P+g9ERGQ+bM4hopqpRQtg3jxZHn8caN0asLcHPv5Yir5NmQLk5cnc4l9+Cbz8svzf11cqqhNRjZB+LRV1GriYOwwiIqrFmHQTUe2iUgGvvQb07i1dzUeNAm7dAn7+GWjcWKY+27kT2L3b3JESkQGkx6XDuZGbucMgIqJajN3Liah2Gj1aupLb2wMBAYXrrayA5cuBoUOBVasK5xBXq4F9+4D8fEnY2f2cyCJkJGTCuV89c4dBRES1GJNuIqq9AgN1r69bV7qYjx8PzJ8P/PAD8O+/QFAQkJsLLFok48YfeYRF14hMbMEC4MCBIiv+PQJkZ2teKlBBBUXz+lyGH555z9l0ARIRERWjUhRFKX8z80pNTYWrqytSUlLg4sJxWURkIn/+CWzbBowcKa3hBa3bKSlAZCSwfTvQpw/g4gLUqSNL9+5A8+ZmDZssS02/xxn9/B55BPj1V8Mfl4iIqBz63uPY0k1EVJoHH5SlOFdXYPp04KWXgKNHgYwMmQw4IwN47jlg40bAzc3k4RLVOnl5gLW1uaMgIiIqE5NuIqLKcnQs2UXd31+S8a+/NktIRLXKxYtSm4GIiKga42BEIiJDCgqSJKBgjnAiMp5Tp4BWrcwdBRERUZmYdBMRGdrs2dLSffmyuSOhWmbp0qXw9/eHg4MDAgMDERMTU+b2ycnJmDx5Mho0aAB7e3u0bNkSW7ZsqdIxTer0aaB1a3NHQUREVCYm3UREhmZjA3z6KTBpkkwxRmQC69atQ1hYGObNm4dDhw6hU6dO6N+/P27evKlz+5ycHPTt2xeXLl3Chg0bcPr0aSxfvhy+vr6VPqbJnTrFpJuIiKo9Vi8nIjKWqCggMRF4/XVzR0LVmKHucYGBgejWrRs+/fRTAIBarYafnx9efPFFzJgxo8T2kZGReO+993Dq1CnY2toa5Ji6GPUe/sgjwC+/FM4sQEREZEL63uPY0k1EZCyhocD//R9w9aq5I6EaLicnBwcPHkRwcLBmnZWVFYKDg7F3716d+2zatAlBQUGYPHkyvL290b59eyxatAj5d3tnVOaYZsGEm4iIqjkm3URExqJSAYsWATNnmjsSquESExORn58Pb29vrfXe3t6Ii4vTuc+FCxewYcMG5OfnY8uWLZgzZw4++OADvPXWW5U+JgBkZ2cjNTVVazGKpCTAw8M4xyYiIjIgJt1ERMbUoYPM6717t7kjIdKiVqtRv359fPHFF+jSpQtGjhyJWbNmITIyskrHjYiIgKurq2bx8/MzUMTFsIgaERFZCCbdRETG9sYbsrCoGhmJp6cnrK2tER8fr7U+Pj4ePj4+Ovdp0KABWrZsCWtra826Nm3aIC4uDjk5OZU6JgCEh4cjJSVFs1y5cqUKZ1YGFlEjIiILwaSbiMjYPDyA4cOBFSvMHQnVUHZ2dujSpQuio6M169RqNaKjoxEUFKRzn169euHcuXNQq9WadWfOnEGDBg1gZ2dXqWMCgL29PVxcXLQWo+Ac3UREZCGYdBMRmcKECcD69cDt2+aOhGqosLAwLF++HKtWrcLJkycxceJEZGRkIDQ0FAAQEhKC8PBwzfYTJ07ErVu3MHXqVJw5cwabN2/GokWLMHnyZL2PaVbnzgHNm5s7CiIionLZmDsAIqJawdoamDsXeO01YPlyVlwmgxs5ciQSEhIwd+5cxMXFISAgAFu3btUUQouNjYWVVeGzdj8/P2zbtg3Tpk1Dx44d4evri6lTp2L69Ol6H9OscnIAe3tzR0FERFQuztNNRGRKH3wApKbKGG8i1Px7nFHOLzcXeOIJ4McfDXM8IiKiSuA83URE1dErr0hBtSVLzB0JkeU6fx5o1szcURAREemFSTcRkam9+aaMR1250tyREFkmFlEjIiILwqSbiMjUVCrg44+BHTsk8c7JMXdERJaFc3QTEZEFYSE1IiJzsLKSKcQ+/hgYNgxwcwMeeQT43/8AJyd538oKUKuBxEQgPh64eRNITgbq1wf8/GRxdjb3mRCZ3qlTQHWooE5ERKQHJt1EROZiaytjvF95BUhIADZvBsLDgexsSbYL5k/28pJE29sbcHWVhGP7duDKFSAtTSqjd+gABAYC3bsDPj7mPS8iY0tIkN8LIiIiC8Ckm4ioOvDyAsaNk6WicnOBY8eAmBhJ2m/ckCJT//sf8MADQL16Bg6WyIwKJl3htHtERGQhKjWme+nSpfD394eDgwMCAwMRExNT6rbHjx/H8OHD4e/vD5VKhSWs2EtEZFi2tkDnzsDzzwNffQX89pu0nt+6BUydKt3WH30UeOklYNkyYMsW4OhRICmpMIEhshRs5SYiIgtT4ZbudevWISwsDJGRkQgMDMSSJUvQv39/nD59GvXr1y+xfWZmJpo2bYoRI0Zg2rRpBgmaiIjKoFIBTZvKMmGCrMvPBy5dkq7ply8Df/8NXL0qY8V9fYGwMKBdO7OGTaQXFlEjIiILU+Gke/HixZgwYQJC7xYwiYyMxObNmxEVFYUZM2aU2L5bt27o1q0bAOh8n4iITMDaWrqc65rb+NQp4MMPpQVxyhSgVy/A3t70MRLpg9OFERGRhalQ0p2Tk4ODBw8iPDxcs87KygrBwcHYu3evwYMjIiITaN0a+PxzafWOjASWLtWexszaGnBwKFysrKRbesFiZydV1AsWa2sgL0/Gmuflyeui7zs5yT52dpLcW1lJS3xenvwfkC7zNjbyfycnwMMDcHeXbal2O30auO8+c0dBRESktwol3YmJicjPz4e3t7fWem9vb5w6dcpgQWVnZyM7O1vzOjU11WDHJiKiUnh7A/PmlVyfny8V1bOygDt3pKq6SlW45OYC6emFi1otCXPBkpdX+F5KihR6y82VY+bkyPFtbCQ5t7aWz8zNLUzaMzNl/HlycmGir6uIVsG64u+XN269tOOVZeNGiZlM7/ZtGTpBRERkIarlN4aIiAi88cYb5g6DiIgASYSdnGQhMrcVK8wdARERUYVUqJ+ep6cnrK2tER8fr7U+Pj4ePgacFzY8PBwpKSma5cqVKwY7NhEREREREZGpVCjptrOzQ5cuXRAdHa1Zp1arER0djaCgIIMFZW9vDxcXF62FiIiIiIiIyNJUuHt5WFgYxo4di65du6J79+5YsmQJMjIyNNXMQ0JC4Ovri4iICABSfO3EiROaP1+7dg1HjhyBs7MzmjdvbsBTISIiIiIiIqpeKpx0jxw5EgkJCZg7dy7i4uIQEBCArVu3aoqrxcbGwqpIddnr16+jc+fOmtfvv/8+3n//fdx///3YuXNn1c+AiIiIiIiIqJpSKUp5ZV3NLzU1Fa6urkhJSWFXcyIiqlFq+j2upp8fERHVXvre4zjhKREREREREZGRMOkmIiIiIiIiMhIm3URERERERERGwqSbiIiIiIiIyEiYdBMREREREREZCZNuIiIiIiIiIiNh0k1ERERERERkJDbmDkAfBVOJp6ammjkSIiIiwyq4txXc62oa3sOJiKim0vcebhFJd1paGgDAz8/PzJEQEREZR1paGlxdXc0dhsHxHk5ERDVdefdwlWIBj9bVajWuX7+OunXrQqVSVfl4qamp8PPzw5UrV+Di4mKACGsfXsOq4zWsOl7DquM1rLqqXkNFUZCWloaGDRvCyqrmjfriPbz64TWsOl7DquM1rDpew6oz1T3cIlq6rays0KhRI4Mf18XFhT+gVcRrWHW8hlXHa1h1vIZVV5VrWBNbuAvwHl598RpWHa9h1fEaVh2vYdUZ+x5e8x6pExEREREREVUTTLqJiIiIiIiIjKRWJt329vaYN28e7O3tzR2KxeI1rDpew6rjNaw6XsOq4zU0LV7vquM1rDpew6rjNaw6XsOqM9U1tIhCakRERERERESWqFa2dBMRERERERGZApNuIiIiIiIiIiNh0k1ERERERERkJEy6iYiIiIiIiIyk1iXdS5cuhb+/PxwcHBAYGIiYmBhzh1RtRUREoFu3bqhbty7q16+PoUOH4vTp01rbZGVlYfLkyahXrx6cnZ0xfPhwxMfHmyni6u/tt9+GSqXCyy+/rFnHa1i+a9eu4amnnkK9evXg6OiIDh064MCBA5r3FUXB3Llz0aBBAzg6OiI4OBhnz541Y8TVS35+PubMmYN77rkHjo6OaNasGd58800UraPJa6jtr7/+wuDBg9GwYUOoVCr89NNPWu/rc71u3bqFMWPGwMXFBW5ubhg/fjzS09NNeBY1D+/h+uM93PB4D68c3sOrhvfwiquW93ClFlm7dq1iZ2enREVFKcePH1cmTJiguLm5KfHx8eYOrVrq37+/8tVXXynHjh1Tjhw5ogwaNEhp3Lixkp6ertnmhRdeUPz8/JTo6GjlwIEDSo8ePZSePXuaMerqKyYmRvH391c6duyoTJ06VbOe17Bst27dUpo0aaKMGzdO2b9/v3LhwgVl27Ztyrlz5zTbvP3224qrq6vy008/Kf/++6/y6KOPKvfcc49y584dM0ZefSxcuFCpV6+e8uuvvyoXL15U1q9frzg7OysfffSRZhteQ21btmxRZs2apWzcuFEBoPz4449a7+tzvQYMGKB06tRJ2bdvn/J///d/SvPmzZXRo0eb+ExqDt7DK4b3cMPiPbxyeA+vOt7DK6463sNrVdLdvXt3ZfLkyZrX+fn5SsOGDZWIiAgzRmU5bt68qQBQdu3apSiKoiQnJyu2trbK+vXrNducPHlSAaDs3bvXXGFWS2lpaUqLFi2U7du3K/fff7/mhs1rWL7p06crvXv3LvV9tVqt+Pj4KO+9955mXXJysmJvb6989913pgix2nv44YeVZ555RmvdY489powZM0ZRFF7D8hS/YetzvU6cOKEAUP755x/NNr/99puiUqmUa9eumSz2moT38KrhPbzyeA+vPN7Dq4738KqpLvfwWtO9PCcnBwcPHkRwcLBmnZWVFYKDg7F3714zRmY5UlJSAAAeHh4AgIMHDyI3N1frmrZu3RqNGzfmNS1m8uTJePjhh7WuFcBrqI9Nmzaha9euGDFiBOrXr4/OnTtj+fLlmvcvXryIuLg4rWvo6uqKwMBAXsO7evbsiejoaJw5cwYA8O+//2L37t0YOHAgAF7DitLneu3duxdubm7o2rWrZpvg4GBYWVlh//79Jo/Z0vEeXnW8h1ce7+GVx3t41fEebljmuofbVC1sy5GYmIj8/Hx4e3trrff29sapU6fMFJXlUKvVePnll9GrVy+0b98eABAXFwc7Ozu4ublpbevt7Y24uDgzRFk9rV27FocOHcI///xT4j1ew/JduHABy5YtQ1hYGGbOnIl//vkHL730Euzs7DB27FjNddL1u81rKGbMmIHU1FS0bt0a1tbWyM/Px8KFCzFmzBgA4DWsIH2uV1xcHOrXr6/1vo2NDTw8PHhNK4H38KrhPbzyeA+vGt7Dq473cMMy1z281iTdVDWTJ0/GsWPHsHv3bnOHYlGuXLmCqVOnYvv27XBwcDB3OBZJrVaja9euWLRoEQCgc+fOOHbsGCIjIzF27FgzR2cZvv/+e6xevRpr1qxBu3btcOTIEbz88sto2LAhryFRLcB7eOXwHl51vIdXHe/hNUOt6V7u6ekJa2vrEhUl4+Pj4ePjY6aoLMOUKVPw66+/4s8//0SjRo006318fJCTk4Pk5GSt7XlNCx08eBA3b97EvffeCxsbG9jY2GDXrl34+OOPYWNjA29vb17DcjRo0ABt27bVWtemTRvExsYCgOY68Xe7dK+99hpmzJiBUaNGoUOHDnj66acxbdo0REREAOA1rCh9rpePjw9u3ryp9X5eXh5u3brFa1oJvIdXHu/hlcd7eNXxHl51vIcblrnu4bUm6bazs0OXLl0QHR2tWadWqxEdHY2goCAzRlZ9KYqCKVOm4Mcff8SOHTtwzz33aL3fpUsX2Nraal3T06dPIzY2ltf0roceegj//fcfjhw5olm6du2KMWPGaP7Ma1i2Xr16lZjm5syZM2jSpAkA4J577oGPj4/WNUxNTcX+/ft5De/KzMyElZX2P/fW1tZQq9UAeA0rSp/rFRQUhOTkZBw8eFCzzY4dO6BWqxEYGGjymC0d7+EVx3t41fEeXnW8h1cd7+GGZbZ7eKXKr1motWvXKvb29srKlSuVEydOKM8995zi5uamxMXFmTu0amnixImKq6ursnPnTuXGjRuaJTMzU7PNCy+8oDRu3FjZsWOHcuDAASUoKEgJCgoyY9TVX9HKp4rCa1iemJgYxcbGRlm4cKFy9uxZZfXq1YqTk5Py7bffarZ5++23FTc3N+Xnn39Wjh49qgwZMqRWT5VR3NixYxVfX1/NdCMbN25UPD09lddff12zDa+htrS0NOXw4cPK4cOHFQDK4sWLlcOHDyuXL19WFEW/6zVgwAClc+fOyv79+5Xdu3crLVq04JRhVcB7eMXwHm4cvIdXDO/hVcd7eMVVx3t4rUq6FUVRPvnkE6Vx48aKnZ2d0r17d2Xfvn3mDqnaAqBz+eqrrzTb3LlzR5k0aZLi7u6uODk5KcOGDVNu3LhhvqAtQPEbNq9h+X755Relffv2ir29vdK6dWvliy++0HpfrVYrc+bMUby9vRV7e3vloYceUk6fPm2maKuf1NRUZerUqUrjxo0VBwcHpWnTpsqsWbOU7OxszTa8htr+/PNPnf/+jR07VlEU/a5XUlKSMnr0aMXZ2VlxcXFRQkNDlbS0NDOcTc3Be7j+eA83Dt7DK4738KrhPbziquM9XKUoilK5NnIiIiIiIiIiKkutGdNNREREREREZGpMuomIiIiIiIiMhEk3ERERERERkZEw6SYiIiIiIiIyEibdREREREREREbCpJuIiIiIiIjISJh0ExERERERERkJk24iIiIiIiIiI2HSTURERERERGQkTLqJiIiIiIiIjIRJNxEREREREZGRMOkmIiIiIiIiMpL/Bx0HVZw+39n7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model=create_model(x_train.shape[1:])\n",
    "    # compile\n",
    "    model.compile(loss={'outputs': 'binary_crossentropy','rri':'mean_squared_error','ampl':'mean_squared_error'},loss_weights={\n",
    "                  'outputs': 1,'rri': 1,'ampl':1}, optimizer='adam', metrics={'outputs':'accuracy'})\n",
    "    filepath='/content/drive/MyDrive/Final Result/Final_performance/BAFNet/model/min_mean_1.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_outputs_accuracy', verbose=1, save_best_only=True,mode='max')\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "    callbacks_list = [lr_scheduler, checkpoint]\n",
    "    time_begin = time.time()\n",
    "    history = model.fit(x_train, [y_train,ampl_train,rri_train], batch_size=128, epochs=100,\n",
    "                        validation_data=(x_val, [y_val,ampl_val,rri_val]),callbacks=callbacks_list)\n",
    "    time_end = time.time()\n",
    "    t = time_end - time_begin\n",
    "    print('time_train:', t)\n",
    "    plot(history.history)\n",
    "\n",
    "#     test\n",
    "#     model= load_model(filepath,custom_objects={'ScaledDotProductAttention':ScaledDotProductAttention})\n",
    "\n",
    "    r= model.evaluate(x_test, [y_test,ampl_test,rri_test])\n",
    "    loss=r[0]\n",
    "    acc=r[-1]\n",
    "    # save prediction score\n",
    "    y_score = model.predict(x_test, batch_size=1024, verbose=1)[0]\n",
    "    roc=roc_auc_score(y_score=y_score,y_true=y_test)\n",
    "    output = pd.DataFrame({\"y_true\": y_test[:, 1], \"y_score\": y_score[:, 1], \"subject\": groups_test})\n",
    "# Export to csv file\n",
    "    output.to_csv(\"/content/drive/MyDrive/Final Result/Final_performance/BAFNet/file_name.csv\", index=False)\n",
    "    y=model.predict(x_test, batch_size=1024, verbose=1)[0]\n",
    "    y_true, y_pred = np.argmax(y_test, axis=-1), np.argmax(y, axis=-1)\n",
    "    C = confusion_matrix(y_true, y_pred, labels=(1, 0))\n",
    "    TP, TN, FP, FN = C[0, 0], C[1, 1], C[1, 0], C[0, 1]\n",
    "    acc, sn, sp = 1. * (TP + TN) / (TP + TN + FP + FN), 1. * TP / (TP + FN), 1. * TN / (TN + FP)\n",
    "    f1=f1_score(y_true, y_pred, average='binary')\n",
    "\n",
    "    print(\"TP:{}, TN:{}, FP:{}, FN:{}, loss{}, acc{}, sn{}, sp{}, f1{}, roc{}\".format(TP, TN, FP, FN,loss, acc, sn, sp, f1, roc))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
